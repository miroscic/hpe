/*
  ____                                   _             _       
 / ___|  ___  _   _ _ __ ___ ___   _ __ | |_   _  __ _(_)_ __  
 \___ \ / _ \| | | | '__/ __/ _ \ | '_ \| | | | |/ _` | | '_ \ 
  ___) | (_) | |_| | | | (_|  __/ | |_) | | |_| | (_| | | | | |
 |____/ \___/ \__,_|_|  \___\___| | .__/|_|\__,_|\__, |_|_| |_|
                                  |_|            |___/         
# A Template for HpePlugin, a Source Plugin
# Generated by the command: C:\Program Files\MADS\usr\local\bin\mads-plugin.exe -t source -d C:\mirrorworld\hpe -i C:\Program Files\MADS\usr\local\bin hpe
# Hostname: unknown
# Current working directory: C:\mirrorworld
# Creation date: 2025-07-14T13:40:38.376+0200
# NOTICE: MADS Version 1.3.1
*/

// Mandatory included headers
#include <source.hpp>
#include <nlohmann/json.hpp>
#include <pugg/Kernel.h>

// other includes as needed here
#include <chrono>
#include <filesystem>
#include <fstream>
#include <iostream>
#include <opencv2/core.hpp>
#include <opencv2/opencv.hpp>
#include <string>

#include <pcl/console/parse.h>
#include <pcl/io/pcd_io.h>
#include <pcl/visualization/pcl_visualizer.h>

#include <Eigen/Dense>
#include <models/hpe_model_openpose.h>
#include <models/input_data.h>
#include <openvino/openvino.hpp>
#include <pipelines/async_pipeline.h>
#include <pipelines/metadata.h>
#include <utils/common.hpp>

#ifdef __linux
#include <lccv.hpp> // for Raspi
#endif

// Define the name of the plugin
#ifndef PLUGIN_NAME
#define PLUGIN_NAME "hpe"
#endif

#ifdef _WIN32
#define _USE_MATH_DEFINES
#include <cmath>
#endif

#ifdef KINECT_AZURE_LIBS
#pragma message("This computer has the Kinect Azure SDK installed.")
// include Kinect libraries
#include <k4a/k4a.h>
#include <k4a/k4a.hpp>
#include <k4abt.hpp>
#include <k4arecord/playback.h>
#endif

// Load the namespaces
using namespace cv;
using namespace std;
using namespace std::chrono;
using json = nlohmann::json;

// Define the name of the plugin
#ifndef PLUGIN_NAME
#define PLUGIN_NAME "hpe"
#endif

typedef float data_t;

struct color_point_t {
  int16_t xyz[3];
  uint8_t rgb[3];
};

// Map of OpenPOSE keypoint names

map<int, string> keypoints_map_openpose = {
  {0, "NOS_"},  {1, "NEC_"},  {2, "SHOR"},  {3, "ELBR"},  {4, "WRIR"},
  {5, "SHOL"},  {6, "ELBL"},  {7, "WRIL"},  {8, "HIPR"},  {9, "KNER"},
  {10, "ANKR"}, {11, "HIPL"}, {12, "KNEL"}, {13, "ANKL"}, {14, "EYER"},
  {15, "EYEL"}, {16, "EARR"}, {17, "EARL"}};

map<int, string> keypoints_map_azure = {
  {27, "NOS_"}, {3, "NEC_"},  {12, "SHOR"}, {13, "ELBR"}, {14, "WRIR"},
  {5, "SHOL"},  {6, "ELBL"},  {7, "WRIL"},  {22, "HIPR"}, {23, "KNER"},
  {24, "ANKR"}, {18, "HIPL"}, {19, "KNEL"}, {20, "ANKL"}, {30, "EYER"},
  {28, "EYEL"}, {31, "EARR"}, {29, "EARL"}};

enum VideoSource {  /** < Video source type to handle different platforms and devices */
    KINECT_AZURE_CAMERA,
    KINECT_AZURE_DUMMY,
    RGB_CAMERA,
    RGB_CAMERA_DUMMY,
    RASPI_RGB_CAMERA,
    RASPI_RGB_CAMERA_DUMMY,
    UNKNOWN
};

string video_source_to_string(VideoSource source) {
    switch (source) {
      case KINECT_AZURE_CAMERA:
        return "KINECT_AZURE_CAMERA";
      case KINECT_AZURE_DUMMY:
        return "KINECT_AZURE_DUMMY";
      case RGB_CAMERA:
        return "RGB_CAMERA";
      case RGB_CAMERA_DUMMY:
        return "RGB_CAMERA_DUMMY";
      case RASPI_RGB_CAMERA:
        return "RASPI_RGB_CAMERA";
      case RASPI_RGB_CAMERA_DUMMY:
        return "RASPI_RGB_CAMERA_DUMMY";
      case UNKNOWN:
        return "UNKNOWN";
      default:
        return "INVALID";
    }
}

// Plugin class. This shall be the only part that needs to be modified,
// implementing the actual functionality
class HpePlugin : public Source<json> {

  /*
    ____  _        _   _                                 _
   / ___|| |_ __ _| |_(_) ___   _ __ ___   ___ _ __ ___ | |__   ___ _ __ ___
   \___ \| __/ _` | __| |/ __| | '_ ` _ \ / _ \ '_ ` _ \| '_ \ / _ \ '__/ __|
    ___) | (_) | |_| | | (__  | | | | | |  __/ | | | | | |_) |  __/ |  \__ \
   |____/ \__\__,_|\__|_|\___| |_| |_| |_|\___|_| |_| |_|_.__/ \___|_|  |___/

  */



  /*
    __  __      _   _               _
   |  \/  | ___| |_| |__   ___   __| |___
   | |\/| |/ _ \ __| '_ \ / _ \ / _` / __|
   | |  | |  __/ |_| | | | (_) | (_| \__ \
   |_|  |_|\___|\__|_| |_|\___/ \__,_|___/

  */

public:

  // Constructor
  HpePlugin() : _agent_id(PLUGIN_NAME) {}

  // Destructor
  ~HpePlugin() {}

  string get_dummy_file_extension(string dummy_file_path = "") {
    // Return the file extension for dummy files
    if (dummy_file_path.empty()) {
      return "";
    }
    
    // Find the last dot in the file path
    size_t dot_pos = dummy_file_path.find_last_of('.');
    if (dot_pos != string::npos) {
      return dummy_file_path.substr(dot_pos);
    }
    
    // If no extension found, return empty string
    return "";
  }

  bool is_raspberry_pi() {
    // Check if the platform is a Raspberry Pi
    ifstream cpuinfo("/proc/cpuinfo");
    string line;
    while (std::getline(cpuinfo, line)) {
      if (line.find("Raspberry Pi") != string::npos) {
        return true;
      }
    }
    return false;
  }

  bool is_rgb_camera_connected() {
    // Check if an RGB camera is connected with opencv
    vector<int> camera_indices = {0, 1, 2, 3, 4, 5}; // Common camera indices
    for (int index : camera_indices) {
      VideoCapture cap(index);
      if (cap.isOpened()) {
        cap.release();
        return true; // Camera is connected
      }
    }
    return false; // No camera found
  }

  bool is_kinect_azure_connected() {
    // Check if a Kinect Azure camera is connected
    #ifdef KINECT_AZURE_LIBS
      uint32_t device_count = k4a_device_get_installed_count();
      if( device_count > 0){
        return true;
      }
      else{
        return false;
      }
    #endif

    return false; 
  }

  bool is_raspi_rgb_camera_connected() {
    // Check if a Raspberry Pi RGB camera is connected
    #ifdef __linux
      _raspi_rgb_camera.options->video_width = rgb_width_read;   // 1280;
      _raspi_rgb_camera.options->video_height = rgb_height_read; // 720;
      _raspi_rgb_camera.options->framerate = 25;
      _raspi_rgb_camera.options->verbose = false;
      if(_raspi_rgb_camera.startVideo())
        return true; // Camera is connected
      else
        return false; // Camera is not connected  
    #endif

    return false; 
  }

  bool is_kinect_azure_lib_installed() {
    // Check if the Kinect Azure library is installed
    #ifdef KINECT_AZURE_LIBS
      return true;
    #endif

    return false;
  }

  return_type set_video_source (bool dummy = false) {

    if (dummy) {
      string dummy_ext = get_dummy_file_extension(_params["dummy_file_path"]);

      if (dummy_ext == ".mp4") {
          _video_source = is_raspberry_pi() ? RASPI_RGB_CAMERA_DUMMY : RGB_CAMERA_DUMMY;
      }
      else if (dummy_ext == ".mkv") {
          _video_source = is_kinect_azure_lib_installed() ? KINECT_AZURE_DUMMY : UNKNOWN;
          if (_video_source == UNKNOWN) {
            cout << "\033[1;31mThe dummy file extension is .mkv but the azure kinect libraries are not installed\033[0m" << endl;
            return return_type::error;
          }
      }
      else {
          _video_source = UNKNOWN;
            cout << "\033[1;31mThe dummy file extension is not valid\033[0m" << endl;
          return return_type::error;
      }
    }
    else {
        if (is_raspberry_pi()) {
            _video_source = is_raspi_rgb_camera_connected() ? RASPI_RGB_CAMERA : UNKNOWN;
            if (_video_source == UNKNOWN) {
                cout << "\033[1;31mThe platform is a Raspberry Pi but no camera is connected\033[0m" << endl;
              return return_type::error;
            }
        }
        else if (is_kinect_azure_lib_installed() && is_kinect_azure_connected()) {
            _video_source = KINECT_AZURE_CAMERA;
        }
        else if (is_rgb_camera_connected()) {
            _video_source = RGB_CAMERA;
        }
        else {
            _video_source = UNKNOWN;
            cout << "\033[1;31mNo camera connected\033[0m" << endl;
            return return_type::error;
        }
    }

    return return_type::success;

  }

  #ifdef KINECT_AZURE_LIBS
  k4a::image transform_color_to_depth_coordinates(k4a_transformation_t transformation_handle, 
                                                  k4a::image color_image, 
                                                  k4a::image depth_image) {
    if (!depth_image.is_valid()) {
      cout << "\033[1;33mWarning: No depth image available, skipping color-to-depth transformation\033[0m" << endl;
      return color_image;
    }
    
    cout << "Depth image available - checking color image format..." << endl;
    
    // Check if the color image is compressed (JPEG) or raw (BGRA32)
    k4a_image_format_t color_format = color_image.get_format();
    cout << "Original color image format: " << color_format << endl;
    
    k4a::image color_image_for_transform;
    
    if (color_format == K4A_IMAGE_FORMAT_COLOR_BGRA32) {
      cout << "Color image is already in BGRA32 format" << endl;
      color_image_for_transform = color_image;
    } else {
      cout << "Color image is compressed (format: " << color_format << "), decompressing first..." << endl;
      
      // Decompress the image to get raw BGRA32 data
      size_t buffer_size = color_image.get_size();
      uint8_t *compressed_buffer = color_image.get_buffer();
      
      // Create a temporary Mat from the compressed buffer
      vector<uint8_t> compressed_data(compressed_buffer, compressed_buffer + buffer_size);
      
      // Decode the compressed image
      cv::Mat decoded_image = cv::imdecode(compressed_data, cv::IMREAD_COLOR);
      
      if (decoded_image.empty()) {
        cout << "\033[1;31mFailed to decode compressed image for transformation!\033[0m" << endl;
        return color_image; // Use original as fallback
      }
      
      cout << "Decoded image dimensions: " << decoded_image.cols << "x" << decoded_image.rows << endl;
      
      // Convert BGR to BGRA
      cv::Mat bgra_image;
      cv::cvtColor(decoded_image, bgra_image, cv::COLOR_BGR2BGRA);
      
      // Create a new k4a::image with the decompressed BGRA data
      color_image_for_transform = k4a::image::create(
        K4A_IMAGE_FORMAT_COLOR_BGRA32,
        bgra_image.cols,
        bgra_image.rows,
        bgra_image.cols * 4 * sizeof(uint8_t)
      );
      
      if (color_image_for_transform.is_valid()) {
        // Copy the BGRA data to the k4a::image
        uint8_t *k4a_buffer = color_image_for_transform.get_buffer();
        memcpy(k4a_buffer, bgra_image.data, bgra_image.total() * bgra_image.elemSize());
        cout << "Created BGRA32 k4a::image for transformation" << endl;
      } else {
        cout << "\033[1;33mWarning: Failed to create BGRA32 k4a::image, using original\033[0m" << endl;
        return color_image;
      }
    }
    
    // Now proceed with the transformation using the BGRA32 image
    if (color_image_for_transform.is_valid() && color_image_for_transform.get_format() == K4A_IMAGE_FORMAT_COLOR_BGRA32) {
      cout << "Proceeding with color to depth transformation..." << endl;
      
      // Create a new k4a::image for the transformed color image
      k4a::image transformed_color_image = k4a::image::create(
        K4A_IMAGE_FORMAT_COLOR_BGRA32,
        depth_image.get_width_pixels(),
        depth_image.get_height_pixels(),
        depth_image.get_width_pixels() * 4 * sizeof(uint8_t)
      );
      
      if (transformed_color_image.is_valid()) {
        // Transform color image to depth camera coordinates
        k4a_result_t transform_result = k4a_transformation_color_image_to_depth_camera(
          transformation_handle,
          depth_image.handle(),
          color_image_for_transform.handle(),
          transformed_color_image.handle()
        );
        
        if (transform_result == K4A_RESULT_SUCCEEDED) {
          cout << "Color to depth transformation successful" << endl;
          cout << "Transformed image dimensions: " << transformed_color_image.get_width_pixels() 
               << "x" << transformed_color_image.get_height_pixels() << endl;
          return transformed_color_image;
        } else {
          cout << "\033[1;33mWarning: Color to depth transformation failed, using original color image\033[0m" << endl;
        }
      } else {
        cout << "\033[1;33mWarning: Failed to create transformed color image buffer\033[0m" << endl;
      }
    } else {
      cout << "\033[1;33mWarning: Cannot transform - color image is not in BGRA32 format\033[0m" << endl;
    }
    
    return color_image; // Return original if transformation failed
  }
  #endif

  #ifdef KINECT_AZURE_LIBS
  return_type k4a_color_image_to_cv_mat(k4a::image k4a_image, cv::Mat& output_mat) {
    
    if (!k4a_image.is_valid()) {
      cout << "\033[1;31mInvalid k4a::image provided\033[0m" << endl;
      return return_type::error;
    }
    
    int rows = k4a_image.get_height_pixels();
    int cols = k4a_image.get_width_pixels();
    
    // Check buffer size
    size_t buffer_size = k4a_image.get_size();
    uint8_t *buffer = k4a_image.get_buffer();
    
    // Calculate expected buffer size
    size_t expected_size = rows * cols * 4; // 4 bytes per pixel for BGRA
    
    // Get the stride (bytes per row) from the image
    size_t stride = k4a_image.get_stride_bytes();
    
    // Check image format
    k4a_image_format_t format = k4a_image.get_format();
    
    if (stride == 0 || buffer_size < expected_size) {
      // Handle compressed image
      vector<uint8_t> compressed_data(buffer, buffer + buffer_size);
      
      // Decode the compressed image
      cv::Mat decoded_image = cv::imdecode(compressed_data, cv::IMREAD_COLOR);
      
      if (decoded_image.empty()) {
        cout << "\033[1;31mFailed to decode compressed image!\033[0m" << endl;
        return return_type::error;
      }

      // Convert to BGR if necessary (OpenCV imdecode usually returns BGR)
      if (decoded_image.channels() == 3) {
        output_mat = decoded_image.clone();
      } else {
        cv::cvtColor(decoded_image, output_mat, cv::COLOR_BGRA2BGR);
      }
    } else {
      // Handle raw BGRA data
      try {
        cv::Mat temp_mat = cv::Mat(rows, cols, CV_8UC4, (void *)buffer, stride);
        cv::cvtColor(temp_mat, output_mat, cv::COLOR_BGRA2BGR);
      } catch (const cv::Exception& e) {
        cout << "\033[1;31mColor conversion failed: " << e.what() << "\033[0m" << endl;
        return return_type::error;
      } catch (const std::exception& e) {
        cout << "\033[1;31mException creating Mat: " << e.what() << "\033[0m" << endl;
        return return_type::error;
      }
    }
    
    // Final check if Mat was created successfully
    if (output_mat.empty()) {
      cout << "\033[1;31mFailed to create cv::Mat from k4a::image!\033[0m" << endl;
      return return_type::error;
    }  

    return return_type::success;
  }
  #endif

  #ifdef KINECT_AZURE_LIBS
  return_type k4a_depth_image_to_cv_mat(k4a::image k4a_depth_image, cv::Mat& output_mat) {
    
    if (!k4a_depth_image.is_valid()) {
      cout << "\033[1;31mInvalid k4a depth image provided\033[0m" << endl;
      return return_type::error;
    }
    
    int rows = k4a_depth_image.get_height_pixels();
    int cols = k4a_depth_image.get_width_pixels();
    
    // Check buffer size
    size_t buffer_size = k4a_depth_image.get_size();
    uint8_t *buffer = k4a_depth_image.get_buffer();
    
    // Calculate expected buffer size for raw depth image (2 bytes per pixel)
    size_t expected_raw_size = rows * cols * 2; // 2 bytes per pixel for 16-bit depth
    
    // Get the stride (bytes per row) from the image
    size_t stride = k4a_depth_image.get_stride_bytes();
    
    // Check image format
    k4a_image_format_t format = k4a_depth_image.get_format();
    
    if (format != K4A_IMAGE_FORMAT_DEPTH16) {
      cout << "\033[1;31mUnsupported depth image format: " << format << "\033[0m" << endl;
      return return_type::error;
    }
    
    // Check if the depth image is compressed (from MKV file)
    if (stride == 0 || buffer_size < expected_raw_size) {
      cout << "Depth image appears to be compressed (buffer size: " << buffer_size 
           << ", expected: " << expected_raw_size << ")" << endl;
      
      // Try to decompress using OpenCV (some MKV files use standard compression)
      vector<uint8_t> compressed_data(buffer, buffer + buffer_size);
      cv::Mat decoded_image = cv::imdecode(compressed_data, cv::IMREAD_ANYDEPTH | cv::IMREAD_ANYCOLOR);
      
      if (!decoded_image.empty()) {
        cout << "Successfully decoded compressed depth image using OpenCV" << endl;
        
        // Convert to 16-bit if necessary
        if (decoded_image.type() == CV_16U) {
          output_mat = decoded_image.clone();
        } else {
          decoded_image.convertTo(output_mat, CV_16U);
        }
      } else {
        cout << "\033[1;33mWarning: Failed to decode compressed depth data with OpenCV\033[0m" << endl;
        cout << "\033[1;33mAssuming raw depth data despite size mismatch\033[0m" << endl;
        
        // Fallback: try to interpret as raw data anyway
        if (stride == 0) {
          stride = cols * 2; // Default stride for 16-bit depth
        }
        
        try {
          cv::Mat temp_mat = cv::Mat(rows, cols, CV_16U, (void *)buffer, stride);
          output_mat = temp_mat.clone();
        } catch (const cv::Exception& e) {
          cout << "\033[1;31mFailed to create Mat from raw depth data: " << e.what() << "\033[0m" << endl;
          return return_type::error;
        }
      }
    } else {
      // Handle raw depth data (16-bit unsigned integers)
      cout << "Processing raw depth data" << endl;
      
      try {
        if (stride == 0) {
          stride = cols * 2; // Default stride for 16-bit depth
        }
        
        // Create Mat from raw depth data
        cv::Mat temp_mat = cv::Mat(rows, cols, CV_16U, (void *)buffer, stride);
        
        // Clone the data to ensure it's owned by output_mat
        output_mat = temp_mat.clone();
        
      } catch (const cv::Exception& e) {
        cout << "\033[1;31mDepth image conversion failed: " << e.what() << "\033[0m" << endl;
        return return_type::error;
      } catch (const std::exception& e) {
        cout << "\033[1;31mException creating depth Mat: " << e.what() << "\033[0m" << endl;
        return return_type::error;
      }
    }
    
    // Final check if Mat was created successfully
    if (output_mat.empty()) {
      cout << "\033[1;31mFailed to create cv::Mat from k4a depth image!\033[0m" << endl;
      return return_type::error;
    }
    
    cout << "Depth image converted successfully: " << output_mat.cols << "x" << output_mat.rows 
         << " (type: " << output_mat.type() << ")" << endl;

    return return_type::success;
  }
  #endif

  return_type setup_video_capture(bool debug = false){

    size_t found = _resolution_rgb.find("x");
    int rgb_width_read = 1280;
    int rgb_height_read = 720;

    if (found != string::npos) {
      cout <<"found? " << found << endl;
      rgb_width_read = stoi(_resolution_rgb.substr(0, found));
      rgb_height_read = stoi(_resolution_rgb.substr(found + 1, _resolution_rgb.length()));
    }

    if (_video_source == KINECT_AZURE_CAMERA) {
      #ifdef KINECT_AZURE_LIBS
      _device_config = K4A_DEVICE_CONFIG_INIT_DISABLE_ALL;
      _device_config.color_format = K4A_IMAGE_FORMAT_COLOR_BGRA32;  
      _device_config.color_resolution = K4A_COLOR_RESOLUTION_720P;
      _device_config.depth_mode = K4A_DEPTH_MODE_NFOV_UNBINNED;
      
      if (_params.contains("azure_device")) {
          _azure_device = _params["azure_device"];
          cout << "Camera id: " << _azure_device << endl;
        } else {
          cout << "Camera id (default): " << _azure_device << endl;
      }

      _kinect_device = k4a::device::open(_azure_device);
      _kinect_device.start_cameras(&_device_config);

      _kinect_calibration = _kinect_device.get_calibration(_device_config.depth_mode, _device_config.color_resolution);
      
      // Get the depth camera intrinsics
      auto depth_camera_calibration = _kinect_calibration.depth_camera_calibration;

      _cx = depth_camera_calibration.intrinsics.parameters.param.cx;
      _cy = depth_camera_calibration.intrinsics.parameters.param.cy;
      _fx = depth_camera_calibration.intrinsics.parameters.param.fx;
      _fy = depth_camera_calibration.intrinsics.parameters.param.fy;

      // Create transformation handle for color-to-depth coordinate transformation
      _kinect_color_transformation_handle = k4a_transformation_create(&_kinect_calibration);
      if (_kinect_color_transformation_handle == nullptr) {
        cout << "\033[1;31mFailed to create transformation handle\033[0m" << endl;
        return return_type::error;
      }
      
      _point_cloud_transformation = k4a_transformation_create(&_kinect_calibration);

      _kinect_tracker_config = K4ABT_TRACKER_CONFIG_DEFAULT;
      if (_params.contains("CUDA")) {
        cout << "Body tracker CUDA processor enabled: " << _params["CUDA"] << endl;
        if (_params["CUDA"] == true)
          _kinect_tracker_config.processing_mode = K4ABT_TRACKER_PROCESSING_MODE_GPU_CUDA;
      }
      _kinect_tracker = k4abt::tracker::create(_kinect_calibration, K4ABT_TRACKER_CONFIG_DEFAULT);

      // acquire a frame just to get the resolution
      _kinect_device.get_capture(&_k4a_rgbd_capture, std::chrono::milliseconds(K4A_WAIT_INFINITE));

      _k4a_color_image = _k4a_rgbd_capture.get_color_image();

      // Transform the color image into depth image coordinates before converting into cv::Mat
      _k4a_depth_image = _k4a_rgbd_capture.get_depth_image();
      _k4a_color_image = transform_color_to_depth_coordinates(_mkv_color_transformation_handle, _k4a_color_image, _k4a_depth_image);

      //TODO: è da fare nel distruttore
      // Clean up the transformation handle
      //k4a_transformation_destroy(_kinect_color_transformation_handle);

      // Convert k4a::image to cv::Mat --> color image

      if (k4a_color_image_to_cv_mat(_k4a_color_image, _rgb) != return_type::success) {
        cout << "\033[1;31mFailed to convert k4a::image to cv::Mat!\033[0m" << endl;
        return return_type::error;
      }

      // Save the RGB image for debugging
      if (debug) {
        string filename = "azure_rgb_transformed_to_depth_coordinates.jpg";
        
        // Create debug directory path relative to the source file location
        std::filesystem::path debug_dir;
        
        // Get the directory of the current source file
        std::filesystem::path source_file_path = std::filesystem::path(__FILE__);
        std::filesystem::path source_dir = source_file_path.parent_path();
        std::filesystem::path hpe_path;
        
        // Look for hpe directory starting from source file directory and going up
        std::filesystem::path search_path = source_dir;
        bool found_hpe = false;
        
        for (int i = 0; i < 5; ++i) { // Search up to 5 levels up
          std::filesystem::path potential_hpe = search_path / "hpe";
          if (std::filesystem::exists(potential_hpe) && std::filesystem::is_directory(potential_hpe)) {
            hpe_path = potential_hpe;
            found_hpe = true;
            break;
          }
          // If we're already in a directory named "hpe", use it
          if (search_path.filename() == "hpe") {
            hpe_path = search_path;
            found_hpe = true;
            break;
          }
          if (search_path.has_parent_path()) {
            search_path = search_path.parent_path();
          } else {
            break;
          }
        }
        
        if (found_hpe) {
          debug_dir = hpe_path / "Debug";
        } else {
          // Fallback: use source directory + Debug
          debug_dir = source_dir / "Debug";
        }
        
        // Create debug directory if it doesn't exist
        try {
          if (!std::filesystem::exists(debug_dir)) {
            std::filesystem::create_directories(debug_dir);
            cout << "Created debug directory: " << debug_dir << endl;
          }
        } catch (const std::filesystem::filesystem_error& e) {
          cout << "\033[1;33mWarning: Failed to create debug directory: " << e.what() << "\033[0m" << endl;
          debug_dir = source_dir; // Use source directory as fallback
        }
        
        string output_filename = (debug_dir / filename).string();
        
        try {
          bool saved = cv::imwrite(output_filename, _rgb);
          if (saved) {
            cout << "\033[1;32mTransformed RGB image saved as: " << output_filename << "\033[0m" << endl;
          } else {
            cout << "\033[1;33mWarning: Failed to save transformed RGB image\033[0m" << endl;
          }
        } catch (const cv::Exception& e) {
          cout << "\033[1;33mException saving transformed RGB image: " << e.what() << "\033[0m" << endl;
        }
      }

      #endif
    }
    else if (_video_source == RGB_CAMERA){

    }
    else if (_video_source == RASPI_RGB_CAMERA){

    }
    else if (_video_source == KINECT_AZURE_DUMMY) {
      #ifdef KINECT_AZURE_LIBS
      _kinect_mkv_playback_handle = nullptr;

      // A C++ Wrapper for the k4a_playback_t does not exist, so we use the C API directly
      if (k4a_playback_open(_params["dummy_file_path"].get<string>().c_str(), &_kinect_mkv_playback_handle) != K4A_RESULT_SUCCEEDED) {
        cout << "\033[1;31mFailed to open dummy file for playback\033[0m" << endl;
        return return_type::error;
      }

      if (k4a_playback_get_calibration(_kinect_mkv_playback_handle, &_kinect_calibration) != K4A_RESULT_SUCCEEDED) {
        cout << "\033[1;31mFailed to get calibration from dummy file\033[0m" << endl;
        return return_type::error;
      }

      // Get the depth camera intrinsics
      auto depth_camera_calibration = _kinect_calibration.depth_camera_calibration;

      _cx = depth_camera_calibration.intrinsics.parameters.param.cx;
      _cy = depth_camera_calibration.intrinsics.parameters.param.cy;
      _fx = depth_camera_calibration.intrinsics.parameters.param.fx;
      _fy = depth_camera_calibration.intrinsics.parameters.param.fy;

      // Create transformation handle for color-to-depth coordinate transformation
      _mkv_color_transformation_handle = k4a_transformation_create(&_kinect_calibration);
      if (_mkv_color_transformation_handle == nullptr) {
        cout << "\033[1;31mFailed to create transformation handle\033[0m" << endl;
        return return_type::error;
      }

      // We can use again the C++ API to create the _kinect_tracker
      _kinect_tracker_config = K4ABT_TRACKER_CONFIG_DEFAULT;
      if (_params.contains("CUDA")) {
        cout << "Body tracker CUDA processor enabled: " << _params["CUDA"] << endl;
        if (_params["CUDA"] == true)
          _kinect_tracker_config.processing_mode = K4ABT_TRACKER_PROCESSING_MODE_GPU_CUDA;
      }
      _kinect_tracker = k4abt::tracker::create(_kinect_calibration, K4ABT_TRACKER_CONFIG_DEFAULT);

      
      if (k4a_playback_get_next_capture(_kinect_mkv_playback_handle, &_kinect_mkv_capture_handle) != K4A_RESULT_SUCCEEDED) {
        cout << "\033[1;31mFailed to get next capture from dummy file\033[0m" << endl;
        return return_type::error;
      }

      // Wrap the C handle in a C++ object
      _k4a_rgbd_capture = k4a::capture(_kinect_mkv_capture_handle);
      
      // Check if the capture is valid
      if (!_k4a_rgbd_capture) {
        cout << "\033[1;31mInvalid capture from dummy file\033[0m" << endl;
        return return_type::error;
      }
      
      _k4a_color_image = _k4a_rgbd_capture.get_color_image();
      
      // Transform the color image into depth image coordinates before converting into cv::Mat
      _k4a_depth_image = _k4a_rgbd_capture.get_depth_image();
      _k4a_color_image = transform_color_to_depth_coordinates(_mkv_color_transformation_handle, _k4a_color_image, _k4a_depth_image);

      //TODO: è da fare nel distruttore
      // Clean up the transformation handle
      //k4a_transformation_destroy(_mkv_color_transformation_handle);

      // Convert k4a::image to cv::Mat --> color image
      if (k4a_color_image_to_cv_mat(_k4a_color_image, _rgb) != return_type::success) {
        cout << "\033[1;31mFailed to convert k4a::image to cv::Mat!\033[0m" << endl;
        return return_type::error;
      }
      
      // Save the RGB image for debugging
      if (debug) {
        string filename = "mkv_rgb_transformed_to_depth_coordinates.jpg";
        
        // Create debug directory path relative to the source file location
        std::filesystem::path debug_dir;
        
        // Get the directory of the current source file
        std::filesystem::path source_file_path = std::filesystem::path(__FILE__);
        std::filesystem::path source_dir = source_file_path.parent_path();
        std::filesystem::path hpe_path;
        
        // Look for hpe directory starting from source file directory and going up
        std::filesystem::path search_path = source_dir;
        bool found_hpe = false;
        
        for (int i = 0; i < 5; ++i) { // Search up to 5 levels up
          std::filesystem::path potential_hpe = search_path / "hpe";
          if (std::filesystem::exists(potential_hpe) && std::filesystem::is_directory(potential_hpe)) {
            hpe_path = potential_hpe;
            found_hpe = true;
            break;
          }
          // If we're already in a directory named "hpe", use it
          if (search_path.filename() == "hpe") {
            hpe_path = search_path;
            found_hpe = true;
            break;
          }
          if (search_path.has_parent_path()) {
            search_path = search_path.parent_path();
          } else {
            break;
          }
        }
        
        if (found_hpe) {
          debug_dir = hpe_path / "Debug";
        } else {
          // Fallback: use source directory + Debug
          debug_dir = source_dir / "Debug";
        }
        
        // Create debug directory if it doesn't exist
        try {
          if (!std::filesystem::exists(debug_dir)) {
            std::filesystem::create_directories(debug_dir);
            cout << "Created debug directory: " << debug_dir << endl;
          }
        } catch (const std::filesystem::filesystem_error& e) {
          cout << "\033[1;33mWarning: Failed to create debug directory: " << e.what() << "\033[0m" << endl;
          debug_dir = source_dir; // Use source directory as fallback
        }
        
        string output_filename = (debug_dir / filename).string();
        
        try {
          bool saved = cv::imwrite(output_filename, _rgb);
          if (saved) {
            cout << "\033[1;32mTransformed RGB image saved as: " << output_filename << "\033[0m" << endl;
          } else {
            cout << "\033[1;33mWarning: Failed to save transformed RGB image\033[0m" << endl;
          }
        } catch (const cv::Exception& e) {
          cout << "\033[1;33mException saving transformed RGB image: " << e.what() << "\033[0m" << endl;
        }
      }
      #endif
    }
    else if (_video_source == RGB_CAMERA_DUMMY || _video_source == RASPI_RGB_CAMERA_DUMMY){

    }
    else {
      cout << "Unknown video source type. Cannot acquire frame." << endl;
      return return_type::error;
    }

    _frame_time = chrono::steady_clock::now();
    cv::Size resolution = _rgb.size();
    cout << "Frame resolution: " << resolution.width << "x" << resolution.height << endl;

    // If KINECT_AZURE don't change the resolution because the rgb image 
    //is in the coordinates of the depth image and MUST stay that way
    if (_video_source != KINECT_AZURE_DUMMY || _video_source != KINECT_AZURE_CAMERA){
      if(found != string::npos){
        resolution = cv::Size(rgb_width_read, rgb_height_read);

        _output_transform = OutputTransform(_rgb.size(), resolution);
        resolution = _output_transform.computeResolution();
        cv::resize(_rgb, _rgb, cv::Size(resolution.width, resolution.height));
      }
    }

    _rgb_height = resolution.height;
    _rgb_width = resolution.width;

    cout << "RGB Frame resolution: " << _rgb_width << "x" << _rgb_height << endl;

    return return_type::success;
  }

  /**
   * @brief Acquire a frame from a camera device. Camera ID is defined in the
   * parameters list.
   *
   * The acquired frame is stored in the #_k4a_rgbd, #_rgbd and #_rgb
   * attributes.
   *
   * @see set_params
   * @author Nicola
   * @return result status ad defined in return_type
   */
  return_type acquire_frame( bool debug = false) {

      cout << "Acquiring frame..." << endl;

    if (_video_source == KINECT_AZURE_CAMERA){
      #ifdef KINECT_AZURE_LIBS

      _kinect_device.get_capture(&_k4a_rgbd_capture, std::chrono::milliseconds(K4A_WAIT_INFINITE));

      _k4a_color_image = _k4a_rgbd_capture.get_color_image();

      // Transform the color image into depth image coordinates before converting into cv::Mat
      _k4a_depth_image = _k4a_rgbd_capture.get_depth_image();
      _k4a_color_image = transform_color_to_depth_coordinates(_kinect_color_transformation_handle, _k4a_color_image, _k4a_depth_image);

      // Convert k4a::image to cv::Mat --> color image
      if (k4a_color_image_to_cv_mat(_k4a_color_image, _rgb) != return_type::success) {
        cout << "\033[1;31mFailed to convert k4a::image to cv::Mat!\033[0m" << endl;
        return return_type::error;
      }

      // Convert k4a::image to cv::Mat --> depth image
      if (k4a_depth_image_to_cv_mat(_k4a_depth_image, _rgbd) != return_type::success) {
        cout << "\033[1;31mFailed to convert k4a::image to cv::Mat!\033[0m" << endl;
        return return_type::error;
      }
    
      #endif
    }
    else if (_video_source == RGB_CAMERA){

    }
    else if (_video_source == RASPI_RGB_CAMERA){

    }
    else if (_video_source == KINECT_AZURE_DUMMY) {
      #ifdef KINECT_AZURE_LIBS

      if (k4a_playback_get_next_capture(_kinect_mkv_playback_handle, &_kinect_mkv_capture_handle) != K4A_RESULT_SUCCEEDED) {
        cout << "\033[1;31mFailed to get next capture from dummy file\033[0m" << endl;
        return return_type::error;
      }

      // Wrap the C handle in a C++ object
      _k4a_rgbd_capture = k4a::capture(_kinect_mkv_capture_handle);
      
      // Check if the capture is valid
      if (!_k4a_rgbd_capture) {
        cout << "\033[1;31mInvalid capture from dummy file\033[0m" << endl;
        return return_type::error;
      }
      
      _k4a_color_image = _k4a_rgbd_capture.get_color_image();
      
      // Transform the color image into depth image coordinates before converting into cv::Mat
      _k4a_depth_image = _k4a_rgbd_capture.get_depth_image();
      _k4a_color_image = transform_color_to_depth_coordinates(_mkv_color_transformation_handle, _k4a_color_image, _k4a_depth_image);

      // Convert k4a::image to cv::Mat --> color image
      if (k4a_color_image_to_cv_mat(_k4a_color_image, _rgb) != return_type::success) {
        cout << "\033[1;31mFailed to convert k4a::image to cv::Mat!\033[0m" << endl;
        return return_type::error;
      }

      // Convert k4a::image to cv::Mat --> depth image
      if (k4a_depth_image_to_cv_mat(_k4a_depth_image, _rgbd) != return_type::success) {
        cout << "\033[1;31mFailed to convert k4a::image to cv::Mat!\033[0m" << endl;
        return return_type::error;
      }
      #endif  
    }
    else if (_video_source == RGB_CAMERA_DUMMY || _video_source == RASPI_RGB_CAMERA_DUMMY){

    }
    else {
      cout << "Unknown video source type. Cannot acquire frame." << endl;
      return return_type::error;
    } 
    
    return return_type::success;
  }

  /**
   * @brief Compute the skeleton from the depth map.
   *
   * Compute the skeleton from the depth map. The resulting skeleton is stored
   * in #_skeleton3D attribute as a map of 3D points.
   *
   * @author Nicola
   * @return result status ad defined in return_type
   */
  return_type skeleton_from_depth_compute(bool debug = false) {

    cout << "Computing skeleton from depth..." << endl;

    if(_video_source == KINECT_AZURE_CAMERA || _video_source == KINECT_AZURE_DUMMY){
      #ifdef KINECT_AZURE_LIBS

      #endif  
    return return_type::success;
    }
  
    return return_type::success;
  }

  /**
   * @brief Remove unnecessary points from the point cloud
   *
   * Make the point cloud lighter by removing unnecessary points, so that it
   * can be sent to the database via network
   *
   * @author Nicola
   * @return result status ad defined in return_type
   */
  return_type point_cloud_filter(bool debug = false) {

    cout << "Filtering point cloud..." << endl;

    if(_video_source == KINECT_AZURE_CAMERA || _video_source == KINECT_AZURE_DUMMY){
      #ifdef KINECT_AZURE_LIBS

      #endif  
    return return_type::success;
    }
    
    return return_type::success;
  }

  /**
   * @brief Transform the 3D skeleton coordinates in the global reference frame
   *
   * Use the extrinsic camera parameters to transorm the 3D skeleton coordinates
   * just before sending them as plugin output.
   *
   * @return return_type
   */
  return_type coordinate_transform(bool debug = false) {
    
    cout << "Transforming coordinates..." << endl;

    return return_type::success;
  }

  /**
   * @brief Compute the skeleton from RGB images only
   *
   * Compute the skeleton from RGB images only. On success, the field
   * #_skeleton2D is updated (as a map of 2D points).
   * Also, the field #_heatmaps is updated with the joints heatmaps (one per
   * joint).
   *
   * There is a configuration flag for optionally skipping this branch
   * on Azure agents.
   *
   * @author Alessandro
   * @return result status ad defined in return_type
   */
  return_type skeleton_from_rgb_compute(bool debug = false) {

    cout << "Computing skeleton from RGB..." << endl;
    
    return return_type::success;
  }

  /**
   * @brief Compute the hessians for joints
   *
   * Compute the hessians for joints on the RGB frame based on the #_heatmaps
   * field.
   *
   * @author Alessandro
   * @return result status ad defined in return_type
   */
  return_type hessian_compute(bool debug = false) {

    cout << "Computing hessians..." << endl;
    
    return return_type::success;
  }

  /**
   * @brief Compute the 3D covariance matrix
   *
   * Compute the 3D covariance matrix.
   * Two possible cases:
   *   1. one Azure camera: use the 3D to uncertainty in the view axis, use
   *      the 2D image to uncertainty in the projection plane
   *   2. one RGB camera: calculates a 3D ellipsoid based on the 2D covariance
   *      plus the "reasonable" depth range as a third azis (direction of view)
   *
   * @author Alessandro
   * @return result status ad defined in return_type
   */
  return_type cov3D_compute(bool debug = false) {

    cout << "Computing 3D covariance..." << endl;
    
    return return_type::success;
  }

  /**
   * @brief Consistency check of the 3D skeleton according to human physiology
   *
   * @authors Marco, Matteo
   * @return result status ad defined in return_type
   */
  return_type consistency_check(bool debug = false) {

    cout << "Performing consistency check..." << endl;
    
    return return_type::success;
  } 

  return_type viewer(bool debug = false) {

    cout << "Launching viewer..." << endl;

    if (_video_source == KINECT_AZURE_CAMERA || _video_source == KINECT_AZURE_DUMMY) {

      // Show _rgb and _rgbd images in two separate windows contempouraneously
      cv::namedWindow("RGB Frame", cv::WINDOW_NORMAL);
      cv::namedWindow("RGBD Frame", cv::WINDOW_NORMAL);

      cv::imshow("RGB Frame", _rgb);

      int MAX_DEPTH = 6000; // Maximum depth value in mm
      _rgbd.convertTo(_rgbd, CV_8U, 255.0 / MAX_DEPTH); 
      Mat rgbd_color;
      applyColorMap(_rgbd, rgbd_color, COLORMAP_HSV);

      cv::imshow("RGBD Frame", rgbd_color);

      // Wait for a key press for 30 milliseconds
      int key = cv::waitKey(30);

      if (key == 27) { // If 'ESC' key is pressed
        cout << "Exiting viewer..." << endl;
        cv::destroyAllWindows();
        return return_type::error;
      }
    }
    
    return return_type::success;
  }

  // Typically, no need to change this
  string kind() override { return PLUGIN_NAME; }

  // Implement the actual functionality here
  return_type get_output(json &out, std::vector<unsigned char> *blob = nullptr) override {

    out.clear();

    out["agent_id"] = _agent_id;
    out["ts"] = std::chrono::duration_cast<std::chrono::nanoseconds>(_frame_time.time_since_epoch()).count();

    if (acquire_frame() == return_type::error) {
      return return_type::error;
    }

    if (skeleton_from_depth_compute(
            _params["debug"]["skeleton_from_depth_compute"]) ==
        return_type::error) {
      return return_type::error;
    }

    if (point_cloud_filter(_params["debug"]["point_cloud_filter"]) ==
        return_type::error) {
      return return_type::error;
    }
    
    if (skeleton_from_rgb_compute(_params["debug"]["skeleton_from_rgb_compute"]) ==
        return_type::error) {
      return return_type::error;
    }

    if (hessian_compute(_params["debug"]["hessian_compute"]) ==
        return_type::error) {
      return return_type::error;
    }

    if (cov3D_compute(_params["debug"]["cov3D_compute"]) ==
        return_type::error) {
      return return_type::error;
    }

    if (viewer(_params["debug"]["viewer"]) == return_type::error) {
      return return_type::error;
    }
        
    if (!_agent_id.empty()) out["agent_id"] = _agent_id;
    return return_type::success;
  }

  void set_params(void const *params) override {
    Source::set_params(params);
    _params.merge_patch(*(json *)params);

    if (_params.contains("resolution_rgb")) {
      _resolution_rgb = _params["resolution_rgb"];
    }

    set_video_source(_params["dummy"]);
    cout << "\033[1;32mVideo source: " << video_source_to_string(_video_source) << "\033[0m" << endl;

    if(setup_video_capture(_params["debug"]["setup_video_capture"]) == return_type::error) {
      cout << "\033[1;31mFailed to setup video capture\033[0m" << endl;
      return;
    }
  } 

  // Implement this method if you want to provide additional information
  map<string, string> info() override { 
    map<string, string> info;
    info["kind"] = kind();
    info["model_file"] = _model_file;
    return info; 
  };

protected:

  string _agent_id; /**< the agent ID */
  string _model_file; /**< the model file path */
  chrono::steady_clock::time_point _frame_time; /**< the timestamp in UNIX [ns] of the last acquired frame */

  Mat _rgbd;          /**< the last RGBD frame */
  Mat _rgb;           /**< the last RGB frame */
  Mat _rgbd_filtered; /**< the last RGBD frame filtered with the body index mask*/

  map<string, vector<unsigned char>> _skeleton2D; /**< the skeleton from 2D cameras only*/
  map<string, vector<float>> _skeleton3D;       /**< the skeleton from 3D cameras only*/
  map<string, Eigen::Matrix3f> _cov3D; /**< the 3D covariance matrix */      
  
  vector<Mat> _heatmaps; /**< the joints heatmaps */
  Mat _point_cloud;      /**< the filtered body point cloud */

  std::vector<Eigen::Matrix2f> _cov2D_vec; /**< the 2D covariance matrix vector */
  std::vector<Eigen::Matrix3f> _cov3D_vec; /**< the 3D covariance matrix vector */
  json _params;   /**< the parameters of the plugin */

  VideoSource _video_source; /**< the video source type */

  #ifdef __linux
  lccv::PiCamera _raspi_rgb_camera; // for Raspi
  #endif

  #ifdef KINECT_AZURE_LIBS
  int _azure_device = 0; /**< the azure device ID */
  k4a_device_configuration_t _device_config; /**< the Kinect Azure device configuration */
  k4a::device _kinect_device; /**< the Kinect Azure device */
  k4a::calibration _kinect_calibration; /**< the Kinect Azure calibration object */
  k4abt::tracker _kinect_tracker; /**< the Kinect Azure tracker */
  k4abt_tracker_configuration_t _kinect_tracker_config; /**< the Kinect Azure tracker configuration */
  k4a::capture _k4a_rgbd_capture; /**< the Kinect Azure RGBD capture */
  k4a_capture_t _kinect_mkv_capture_handle;
  k4a::image _k4a_depth_image; /**< the Kinect Azure depth image */
  k4a::image _k4a_color_image; /**< the Kinect Azure color image */
  k4a_transformation_t _point_cloud_transformation; /**< the Kinect Azure point cloud transformation */
  k4a_transformation_t _mkv_color_transformation_handle; /**< the Kinect Azure MKV color transformation handle */
  k4a_transformation_t _kinect_color_transformation_handle; /**< the Kinect Azure color transformation handle */
  k4a_calibration_intrinsic_parameters_t _k4a_rgb_intrinsics; /**< the Kinect Azure RGB camera intrinsics */
  k4a_playback_t _kinect_mkv_playback_handle; /**< the Kinect Azure MKV playback handle */
  #endif

  float _cx;
  float _cy;
  float _fx;
  float _fy;

  string _resolution_rgb = ""; /**< the resolution of the RGB frame in the format "widthxheight" */
  int _rgb_height; /**< the height of the RGB frame */
  int _rgb_width; /**< the width of the RGB frame */

  OutputTransform _output_transform;

};


/*
  ____  _             _             _      _
 |  _ \| |_   _  __ _(_)_ __     __| |_ __(_)_   _____ _ __
 | |_) | | | | |/ _` | | '_ \   / _` | '__| \ \ / / _ \ '__|
 |  __/| | |_| | (_| | | | | | | (_| | |  | |\ V /  __/ |
 |_|   |_|\__,_|\__, |_|_| |_|  \__,_|_|  |_| \_/ \___|_|
                |___/
Enable the class as plugin
*/
INSTALL_SOURCE_DRIVER(HpePlugin, json)


/*
                  _
  _ __ ___   __ _(_)_ __
 | '_ ` _ \ / _` | | '_ \
 | | | | | | (_| | | | | |
 |_| |_| |_|\__,_|_|_| |_|

For testing purposes, when directly executing the plugin
*/
int main(int argc, char const *argv[]) {
  HpePlugin plugin;
  json output, params;

  // Set example values to params
  params["test"] = "value";

  // Set the parameters
  plugin.set_params(&params);

  // Process data
  plugin.get_output(output);

  // Produce output
  cout << "Output: " << output << endl;

  return 0;
}
