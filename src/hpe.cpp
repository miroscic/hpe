/*
  ____                                   _             _       
 / ___|  ___  _   _ _ __ ___ ___   _ __ | |_   _  __ _(_)_ __  
 \___ \ / _ \| | | | '__/ __/ _ \ | '_ \| | | | |/ _` | | '_ \ 
  ___) | (_) | |_| | | | (_|  __/ | |_) | | |_| | (_| | | | | |
 |____/ \___/ \__,_|_|  \___\___| | .__/|_|\__,_|\__, |_|_| |_|
                                  |_|            |___/         
# A Template for HpePlugin, a Source Plugin
# Generated by the command: C:\Program Files\MADS\usr\local\bin\mads-plugin.exe -t source -d C:\mirrorworld\hpe -i C:\Program Files\MADS\usr\local\bin hpe
# Hostname: unknown
# Current working directory: C:\mirrorworld
# Creation date: 2025-07-14T13:40:38.376+0200
# NOTICE: MADS Version 1.3.1
*/

// Mandatory included headers
#include <source.hpp>
#include <nlohmann/json.hpp>
#include <pugg/Kernel.h>

// other includes as needed here
#include <chrono>
#include <filesystem>
#include <fstream>
#include <iomanip>
#include <iostream>
#include <opencv2/core.hpp>
#include <opencv2/opencv.hpp>
#include <string>
#include <thread>

#include <pcl/console/parse.h>
#include <pcl/io/pcd_io.h>
#include <pcl/visualization/pcl_visualizer.h>

#include <Eigen/Dense>
#include <models/hpe_model_openpose.h>
#include <models/input_data.h>
#include <openvino/openvino.hpp>
#include <pipelines/async_pipeline.h>
#include <pipelines/metadata.h>
#include <utils/common.hpp>

#ifdef __linux
#include <lccv.hpp> // for Raspi
#endif

// Define the name of the plugin
#ifndef PLUGIN_NAME
#define PLUGIN_NAME "hpe"
#endif

#ifdef _WIN32
#define _USE_MATH_DEFINES
#include <cmath>
#endif

#ifdef KINECT_AZURE_LIBS
#pragma message("This computer has the Kinect Azure SDK installed.")
// include Kinect libraries
#include <k4a/k4a.h>
#include <k4a/k4a.hpp>
#include <k4abt.hpp>
#include <k4arecord/playback.h>
#endif

// Load the namespaces
using namespace cv;
using namespace std;
using namespace std::chrono;
using json = nlohmann::json;

// Define the name of the plugin
#ifndef PLUGIN_NAME
#define PLUGIN_NAME "hpe"
#endif

typedef float data_t;

struct color_point_t {
  int16_t xyz[3];
  uint8_t rgb[3];
};

// Map of OpenPOSE keypoint names

map<int, string> keypoints_map_openpose = {
  {0, "NOS_"},  {1, "NEC_"},  {2, "SHOR"},  {3, "ELBR"},  {4, "WRIR"},
  {5, "SHOL"},  {6, "ELBL"},  {7, "WRIL"},  {8, "HIPR"},  {9, "KNER"},
  {10, "ANKR"}, {11, "HIPL"}, {12, "KNEL"}, {13, "ANKL"}, {14, "EYER"},
  {15, "EYEL"}, {16, "EARR"}, {17, "EARL"}};

map<int, string> keypoints_map_azure = {
  {27, "NOS_"}, {3, "NEC_"},  {12, "SHOR"}, {13, "ELBR"}, {14, "WRIR"},
  {5, "SHOL"},  {6, "ELBL"},  {7, "WRIL"},  {22, "HIPR"}, {23, "KNER"},
  {24, "ANKR"}, {18, "HIPL"}, {19, "KNEL"}, {20, "ANKL"}, {30, "EYER"},
  {28, "EYEL"}, {31, "EARR"}, {29, "EARL"}};

enum VideoSource {  /** < Video source type to handle different platforms and devices */
    KINECT_AZURE_CAMERA,
    KINECT_AZURE_DUMMY,
    RGB_CAMERA,
    RGB_CAMERA_DUMMY,
    RASPI_RGB_CAMERA,
    RASPI_RGB_CAMERA_DUMMY,
    UNKNOWN
};

string video_source_to_string(VideoSource source) {
    switch (source) {
      case KINECT_AZURE_CAMERA:
        return "KINECT_AZURE_CAMERA";
      case KINECT_AZURE_DUMMY:
        return "KINECT_AZURE_DUMMY";
      case RGB_CAMERA:
        return "RGB_CAMERA";
      case RGB_CAMERA_DUMMY:
        return "RGB_CAMERA_DUMMY";
      case RASPI_RGB_CAMERA:
        return "RASPI_RGB_CAMERA";
      case RASPI_RGB_CAMERA_DUMMY:
        return "RASPI_RGB_CAMERA_DUMMY";
      case UNKNOWN:
        return "UNKNOWN";
      default:
        return "INVALID";
    }
}

// Plugin class. This shall be the only part that needs to be modified,
// implementing the actual functionality
class HpePlugin : public Source<json> {

  /*
    ____  _        _   _                                 _
   / ___|| |_ __ _| |_(_) ___   _ __ ___   ___ _ __ ___ | |__   ___ _ __ ___
   \___ \| __/ _` | __| |/ __| | '_ ` _ \ / _ \ '_ ` _ \| '_ \ / _ \ '__/ __|
    ___) | (_) | |_| | | (__  | | | | | |  __/ | | | | | |_) |  __/ |  \__ \
   |____/ \__\__,_|\__|_|\___| |_| |_| |_|\___|_| |_| |_|_.__/ \___|_|  |___/

  */

  static cv::Mat renderHumanPose(HumanPoseResult &_result,
                                 OutputTransform &outputTransform) {
    if (!_result.metaData) {
      throw invalid_argument("ERROR: Renderer: metadata is null");
    }

    auto output_img = _result.metaData->asRef<ImageMetaData>().img;

    if (output_img.empty()) {
      throw invalid_argument(
          "ERROR: Renderer: image provided in metadata is empty");
    }
    outputTransform.resize(output_img);
    static const cv::Scalar colors[HPEOpenPose::keypointsNumber] = {
        cv::Scalar(255, 0, 0),   cv::Scalar(255, 85, 0),
        cv::Scalar(255, 170, 0), cv::Scalar(255, 255, 0),
        cv::Scalar(170, 255, 0), cv::Scalar(85, 255, 0),
        cv::Scalar(0, 255, 0),   cv::Scalar(0, 255, 85),
        cv::Scalar(0, 255, 170), cv::Scalar(0, 255, 255),
        cv::Scalar(0, 170, 255), cv::Scalar(0, 85, 255),
        cv::Scalar(0, 0, 255),   cv::Scalar(85, 0, 255),
        cv::Scalar(170, 0, 255), cv::Scalar(255, 0, 255),
        cv::Scalar(255, 0, 170), cv::Scalar(255, 0, 85)};
    static const pair<int, int> keypointsOP[] = {
        {1, 2}, {1, 5},  {2, 3},   {3, 4},  {5, 6},   {6, 7},
        {1, 8}, {8, 9},  {9, 10},  {1, 11}, {11, 12}, {12, 13},
        {1, 0}, {0, 14}, {14, 16}, {0, 15}, {15, 17}};
    static const pair<int, int> keypointsAE[] = {
        {15, 13}, {13, 11}, {16, 14}, {14, 12}, {11, 12}, {5, 11}, {6, 12},
        {5, 6},   {5, 7},   {6, 8},   {7, 9},   {8, 10},  {1, 2},  {0, 1},
        {0, 2},   {1, 3},   {2, 4},   {3, 5},   {4, 6}};
    const int stick_width = 4;
    const cv::Point2f absent_keypoint(-1.0f, -1.0f);
    for (auto &pose : _result.poses) {
      for (size_t keypoint_idx = 0; keypoint_idx < pose.keypoints.size();
           keypoint_idx++) {
        if (pose.keypoints[keypoint_idx] != absent_keypoint) {
          outputTransform.scaleCoord(pose.keypoints[keypoint_idx]);
          cv::circle(output_img, pose.keypoints[keypoint_idx], 4,
                     colors[keypoint_idx], -1);
        }
      }
    }
    vector<pair<int, int>> limb_keypoints_ids;
    if (!_result.poses.empty()) {
      if (_result.poses[0].keypoints.size() == HPEOpenPose::keypointsNumber) {
        limb_keypoints_ids.insert(limb_keypoints_ids.begin(),
                                  begin(keypointsOP), end(keypointsOP));
      } else {
        limb_keypoints_ids.insert(limb_keypoints_ids.begin(),
                                  begin(keypointsAE), end(keypointsAE));
      }
    }
    cv::Mat pane = output_img.clone();
    for (auto pose : _result.poses) {
      for (const auto &limb_keypoints_id : limb_keypoints_ids) {
        pair<cv::Point2f, cv::Point2f> limb_keypoints(
            pose.keypoints[limb_keypoints_id.first],
            pose.keypoints[limb_keypoints_id.second]);
        if (limb_keypoints.first == absent_keypoint ||
            limb_keypoints.second == absent_keypoint) {
          continue;
        }

        data_t mean_x = (limb_keypoints.first.x + limb_keypoints.second.x) / 2;
        data_t mean_y = (limb_keypoints.first.y + limb_keypoints.second.y) / 2;
        cv::Point difference = limb_keypoints.first - limb_keypoints.second;
        data_t length =
            sqrt(difference.x * difference.x + difference.y * difference.y);
        int angle =
            static_cast<int>(atan2(difference.y, difference.x) * 180 / CV_PI);
        vector<cv::Point> polygon;
        cv::ellipse2Poly(cv::Point2d(mean_x, mean_y),
                         cv::Size2d(length / 2, stick_width), angle, 0, 360, 1,
                         polygon);
        cv::fillConvexPoly(pane, polygon, colors[limb_keypoints_id.second]);
      }
    }
    cv::addWeighted(output_img, 0.4, pane, 0.6, 0, output_img);
    return output_img;
  }

  /*
    __  __      _   _               _
   |  \/  | ___| |_| |__   ___   __| |___
   | |\/| |/ _ \ __| '_ \ / _ \ / _` / __|
   | |  | |  __/ |_| | | | (_) | (_| \__ \
   |_|  |_|\___|\__|_| |_|\___/ \__,_|___/

  */

public:

  // Constructor
  HpePlugin() : _agent_id(PLUGIN_NAME) {}

  // Destructor
  ~HpePlugin() {

    if(_video_source == KINECT_AZURE_CAMERA){
      #ifdef KINECT_AZURE_LIBS

      #endif
    }
    else if (_video_source == RGB_CAMERA){

    }
    else if (_video_source == RASPI_RGB_CAMERA){
      #ifdef __linux
        
      #endif
    }
    else if(_video_source == KINECT_AZURE_DUMMY){
      #ifdef KINECT_AZURE_LIBS

      #endif
    }
    else if(_video_source == RGB_CAMERA_DUMMY || _video_source == RASPI_RGB_CAMERA_DUMMY){

    }

    delete _pipeline;
  }

 
  std::filesystem::path get_source_path() {
    
    // Get the directory of the current source file
    std::filesystem::path source_file_path = std::filesystem::path(__FILE__);
    std::filesystem::path source_dir = source_file_path.parent_path();
    std::filesystem::path hpe_path;
    
    // Look for hpe directory starting from source file directory and going up
    std::filesystem::path search_path = source_dir;
    bool found_hpe = false;
    
    for (int i = 0; i < 5; ++i) { // Search up to 5 levels up
      std::filesystem::path potential_hpe = search_path / "hpe";
      if (std::filesystem::exists(potential_hpe) && std::filesystem::is_directory(potential_hpe)) {
        hpe_path = potential_hpe;
        found_hpe = true;
        break;
      }
      // If we're already in a directory named "hpe", use it
      if (search_path.filename() == "hpe") {
        hpe_path = search_path;
        found_hpe = true;
        break;
      }
      if (search_path.has_parent_path()) {
        search_path = search_path.parent_path();
      } else {
        break;
      }
    }
    
    if (found_hpe) {
      return hpe_path;
    } else {
      // Fallback: use source directory + Debug + Covariances 3D
      return source_dir;
    }
  }

  string get_dummy_file_extension(string dummy_file_path = "") {
    // Return the file extension for dummy files
    if (dummy_file_path.empty()) {
      return "";
    }
    
    // Find the last dot in the file path
    size_t dot_pos = dummy_file_path.find_last_of('.');
    if (dot_pos != string::npos) {
      return dummy_file_path.substr(dot_pos);
    }
    
    // If no extension found, return empty string
    return "";
  }

  bool is_raspberry_pi() {
    // Check if the platform is a Raspberry Pi
    ifstream cpuinfo("/proc/cpuinfo");
    string line;
    while (std::getline(cpuinfo, line)) {
      if (line.find("Raspberry Pi") != string::npos) {
        return true;
      }
    }
    return false;
  }

  bool is_rgb_camera_connected() {
    // Check if an RGB camera is connected with opencv
    vector<int> camera_indices = {0, 1, 2, 3, 4, 5}; // Common camera indices
    for (int index : camera_indices) {
      VideoCapture cap(index);
      if (cap.isOpened()) {
        cap.release();
        return true; // Camera is connected
      }
    }
    return false; // No camera found
  }

  bool is_kinect_azure_connected() {
    // Check if a Kinect Azure camera is connected
    #ifdef KINECT_AZURE_LIBS
      uint32_t device_count = k4a_device_get_installed_count();
      if( device_count > 0){
        return true;
      }
      else{
        return false;
      }
    #endif

    return false; 
  }

  bool is_raspi_rgb_camera_connected() {
    // Check if a Raspberry Pi RGB camera is connected
    #ifdef __linux
      _raspi_rgb_camera.options->video_width = _rgb_width_read;   // 1280;
      _raspi_rgb_camera.options->video_height = _rgb_height_read; // 720;
      _raspi_rgb_camera.options->framerate = 25;
      _raspi_rgb_camera.options->verbose = false;
      if(_raspi_rgb_camera.startVideo())
        return true; // Camera is connected
      else
        return false; // Camera is not connected  
    #endif

    return false; 
  }

  bool is_kinect_azure_lib_installed() {
    // Check if the Kinect Azure library is installed
    #ifdef KINECT_AZURE_LIBS
      return true;
    #endif

    return false;
  }

  return_type set_video_source (bool dummy = false) {

    if (dummy) {
      // If dummy mode is enabled, check for dummy files in the dummy directory
      std::filesystem::path dummy_dir = get_source_path();
      dummy_dir = dummy_dir / "dummy";

      cout << "\033[1;33mSearching for dummy files in: " << dummy_dir << "\033[0m" << endl;

      bool found_dummy = false;
      string found_file = "";
      for (const auto &entry : std::filesystem::directory_iterator(dummy_dir)) {
        if (entry.is_regular_file()) {
          string ext = entry.path().extension().string();
          if (ext == ".mp4" || ext == ".mkv") {
            found_dummy = true;
            found_file = entry.path().string();
            break;
          }
        }
      }
      if (!found_dummy) {
        cout << "\033[1;31mNo .mp4 or .mkv dummy file found in: " << dummy_dir << "\033[0m" << endl;
        return return_type::error;
      }
      _params["dummy_file_path"] = found_file;
      cout << "\033[1;32mUsing dummy file: " << _params["dummy_file_path"] << "\033[0m" << endl;

      string dummy_ext = get_dummy_file_extension(_params["dummy_file_path"]); // ridondante! Lo facciamo gia' sopra. Adattare

      if (dummy_ext == ".mp4") {
          _video_source = is_raspberry_pi() ? RASPI_RGB_CAMERA_DUMMY : RGB_CAMERA_DUMMY;
      }
      else if (dummy_ext == ".mkv") {
          _video_source = is_kinect_azure_lib_installed() ? KINECT_AZURE_DUMMY : UNKNOWN;
          if (_video_source == UNKNOWN) {
            cout << "\033[1;31mThe dummy file extension is .mkv but the azure kinect libraries are not installed\033[0m" << endl;
            return return_type::error;
          }
      }
      else {
          _video_source = UNKNOWN;
            cout << "\033[1;31mThe dummy file extension is not valid\033[0m" << endl;
          return return_type::error;
      }
    }
    else {
        if (is_raspberry_pi()) {
            _video_source = is_raspi_rgb_camera_connected() ? RASPI_RGB_CAMERA : UNKNOWN;
            if (_video_source == UNKNOWN) {
                cout << "\033[1;31mThe platform is a Raspberry Pi but no camera is connected\033[0m" << endl;
              return return_type::error;
            }
        }
        else if (is_kinect_azure_lib_installed() && is_kinect_azure_connected()) {
            _video_source = KINECT_AZURE_CAMERA;
        }
        else if (is_rgb_camera_connected()) {
            _video_source = RGB_CAMERA;
        }
        else {
            _video_source = UNKNOWN;
            cout << "\033[1;31mNo camera connected\033[0m" << endl;
            return return_type::error;
        }
    }

    return return_type::success;

  }

  void setup_OpenPoseModel() {
    // Check if model file exists
    if (!std::filesystem::exists(_model_file)) {
      cout << "\033[1;31mError: Model file does not exist: " << _model_file << "\033[0m" << endl;
      return;
    }
    
    // Set default target size if not set
    if (_tsize == 0) {
      _tsize = 416; // Common default for OpenPose models
    }
    
    try {
      // setup inference model
      data_t aspect_ratio = _rgb_width / static_cast<data_t>(_rgb_height);
      
      _model.reset(new HPEOpenPose(_model_file, aspect_ratio, _tsize,
                                   static_cast<data_t>(_threshold), _layout));
      
      if (!_model) {
        cout << "\033[1;31mError: Failed to create HPEOpenPose model\033[0m" << endl;
        return;
      }
      
    } catch (const std::exception& e) {
      cout << "\033[1;31mException in setup_OpenPoseModel: " << e.what() << "\033[0m" << endl;
      _model.reset();
    }
  }

  void setup_Pipeline() {
    // Check if model is properly initialized
    if (!_model) {
      cout << "\033[1;31mError: Model is not initialized before creating pipeline\033[0m" << endl;
      return;
    }
    
    try {
      // setup pipeline
      _pipeline =
          new AsyncPipeline(std::move(_model),
                            ConfigFactory::getUserConfig(
                                _inference_device, _nireq, _nstreams, _nthreads),
                            _core);
      
      if (!_pipeline) {
        cout << "\033[1;31mError: Failed to create AsyncPipeline\033[0m" << endl;
        return;
      }
      
      cout << "Pipeline created successfully" << endl;
      
      // Only submit initial data if we have a valid RGB image
      if (!_rgb.empty()) {
        _frame_num = _pipeline->submitData(
            ImageInputData(_rgb), make_shared<ImageMetaData>(_rgb, _frame_time));
      } else {
        cout << "\033[1;33mWarning: No RGB image available for initial pipeline submission\033[0m" << endl;
        _frame_num = 0;
      }
      
    } catch (const std::exception& e) {
      cout << "\033[1;31mException in setup_Pipeline: " << e.what() << "\033[0m" << endl;
      _pipeline = nullptr;
    }
  }

  return_type setup_video_capture(bool debug = false) {

    size_t found = _resolution_rgb.find("x");

    if (found != string::npos) {
      _rgb_width_read = stoi(_resolution_rgb.substr(0, found));
      _rgb_height_read = stoi(_resolution_rgb.substr(found + 1, _resolution_rgb.length()));
    }

    if (_video_source == KINECT_AZURE_CAMERA) {
      #ifdef KINECT_AZURE_LIBS
      _device_config = K4A_DEVICE_CONFIG_INIT_DISABLE_ALL;
      _device_config.color_format = K4A_IMAGE_FORMAT_COLOR_BGRA32;  
      _device_config.color_resolution = K4A_COLOR_RESOLUTION_720P;
      _device_config.depth_mode = K4A_DEPTH_MODE_NFOV_UNBINNED;
      
      if (_params.contains("azure_device")) {
          _azure_device = _params["azure_device"];
          cout << "Camera id: " << _azure_device << endl;
        } else {
          cout << "Camera id (default): " << _azure_device << endl;
      }

      _kinect_device = k4a::device::open(_azure_device);
      _kinect_device.start_cameras(&_device_config);

      _agent_id = _kinect_device.get_serialnum();

      _kinect_calibration = _kinect_device.get_calibration(_device_config.depth_mode, _device_config.color_resolution);
      
      // Get the depth camera intrinsics
      auto depth_camera_calibration = _kinect_calibration.depth_camera_calibration;

      _cx = depth_camera_calibration.intrinsics.parameters.param.cx;
      _cy = depth_camera_calibration.intrinsics.parameters.param.cy;
      _fx = depth_camera_calibration.intrinsics.parameters.param.fx;
      _fy = depth_camera_calibration.intrinsics.parameters.param.fy;

      // Create transformation handle for color-to-depth coordinate transformation
      _kinect_color_transformation_handle = k4a_transformation_create(&_kinect_calibration);
      if (_kinect_color_transformation_handle == nullptr) {
        cout << "\033[1;31mFailed to create transformation handle\033[0m" << endl;
        return return_type::error;
      }
      
      _point_cloud_transformation = k4a_transformation_create(&_kinect_calibration);

      _kinect_tracker_config = K4ABT_TRACKER_CONFIG_DEFAULT;
      if (_params.contains("CUDA")) {
        if (_params["CUDA"] == true) {
          cout << "Body tracker CUDA processor enabled" << endl;
          _kinect_tracker_config.processing_mode = K4ABT_TRACKER_PROCESSING_MODE_GPU_CUDA;
        } else {
          cout << "Body tracker CUDA processor disabled! Using CPU instead." << endl;
          _kinect_tracker_config.processing_mode = K4ABT_TRACKER_PROCESSING_MODE_CPU;
        }
      }
      cout << "Body tracker processing mode: " << _kinect_tracker_config.processing_mode << endl;
      
      _kinect_tracker = k4abt::tracker::create(_kinect_calibration, _kinect_tracker_config);

      // acquire a frame just to get the resolution
      _kinect_device.get_capture(&_k4a_rgbd_capture, std::chrono::milliseconds(K4A_WAIT_INFINITE));

      _k4a_color_image = _k4a_rgbd_capture.get_color_image();
      
      // Validate the color image before transformation
      if (!_k4a_color_image.is_valid()) {
        cout << "\033[1;31mError: Failed to get valid color image from capture\033[0m" << endl;
        return return_type::error;
      }

      // Transform the color image into depth image coordinates before converting into cv::Mat
      _k4a_depth_image = _k4a_rgbd_capture.get_depth_image();
      
      // Validate the depth image before transformation
      if (!_k4a_depth_image.is_valid()) {
        cout << "\033[1;31mError: Failed to get valid depth image from capture\033[0m" << endl;
        return return_type::error;
      }
      
      _k4a_color_image = transform_color_to_depth_coordinates(_kinect_color_transformation_handle, _k4a_color_image, _k4a_depth_image);

      //TODO: è da fare nel distruttore
      // Clean up the transformation handle
      //k4a_transformation_destroy(_kinect_color_transformation_handle);

      // Convert k4a::image to cv::Mat --> color image

      if (k4a_color_image_to_cv_mat(_k4a_color_image, _rgb) != return_type::success) {
        cout << "\033[1;31mFailed to convert k4a::image to cv::Mat!\033[0m" << endl;
        return return_type::error;
      }

      // Save the RGB image for debugging
      if (debug) {
        string filename = "azure_rgb_transformed_to_depth_coordinates.jpg";
        
        // Create debug directory path relative to the source file location
        std::filesystem::path debug_dir;
        
        // Get the directory of the current source file
        std::filesystem::path source_file_path = std::filesystem::path(__FILE__);
        std::filesystem::path source_dir = source_file_path.parent_path();
        std::filesystem::path hpe_path;
        
        // Look for hpe directory starting from source file directory and going up
        std::filesystem::path search_path = source_dir;
        bool found_hpe = false;
        
        for (int i = 0; i < 5; ++i) { // Search up to 5 levels up
          std::filesystem::path potential_hpe = search_path / "hpe";
          if (std::filesystem::exists(potential_hpe) && std::filesystem::is_directory(potential_hpe)) {
            hpe_path = potential_hpe;
            found_hpe = true;
            break;
          }
          // If we're already in a directory named "hpe", use it
          if (search_path.filename() == "hpe") {
            hpe_path = search_path;
            found_hpe = true;
            break;
          }
          if (search_path.has_parent_path()) {
            search_path = search_path.parent_path();
          } else {
            break;
          }
        }
        
        if (found_hpe) {
          debug_dir = hpe_path / "Debug";
        } else {
          // Fallback: use source directory + Debug
          debug_dir = source_dir / "Debug";
        }
        
        // Create debug directory if it doesn't exist
        try {
          if (!std::filesystem::exists(debug_dir)) {
            std::filesystem::create_directories(debug_dir);
            cout << "Created debug directory: " << debug_dir << endl;
          }
        } catch (const std::filesystem::filesystem_error& e) {
          cout << "\033[1;33mWarning: Failed to create debug directory: " << e.what() << "\033[0m" << endl;
          debug_dir = source_dir; // Use source directory as fallback
        }
        
        string output_filename = (debug_dir / filename).string();
        
        try {
          bool saved = cv::imwrite(output_filename, _rgb);
          if (saved) {
            cout << "\033[1;32mTransformed RGB image saved as: " << output_filename << "\033[0m" << endl;
          } else {
            cout << "\033[1;33mWarning: Failed to save transformed RGB image\033[0m" << endl;
          }
        } catch (const cv::Exception& e) {
          cout << "\033[1;33mException saving transformed RGB image: " << e.what() << "\033[0m" << endl;
        }
      }

      #endif
    }
    else if (_video_source == RGB_CAMERA)
    {
      // setup video capture
      _cap.open(_camera_device);
      if (!_cap.isOpened())
      {
        throw invalid_argument("ERROR: Cannot open the video camera");
      }
      _cap >> _rgb;
    }
    else if (_video_source == RASPI_RGB_CAMERA)
    {
      std::cout << "It is running on a Raspi" << std::endl;
      // _camera.options->video_width = rgb_width_read;   // 1280;
      // _camera.options->video_height = rgb_height_read; // 720;
      // _camera.options->framerate = 25;
      // _camera.options->verbose = false;
      #ifdef __linux
      _raspi_rgb_camera.startVideo();

      do
      {
        if (!_raspi_rgb_camera.getVideoFrame(_rgb, 100))
        {
          std::cout << "Waiting for video frame..." << std::endl;
        }
      } while (_rgb.empty());
      #endif
    }
    else if (_video_source == KINECT_AZURE_DUMMY) {
      #ifdef KINECT_AZURE_LIBS
      _kinect_mkv_playback_handle = nullptr;
      
      // Get camera id from txt file
      std::filesystem::path camera_id_path = std::filesystem::path(_params["dummy_file_path"].get<string>()).parent_path() / "camera_id.txt";
      if (std::filesystem::exists(camera_id_path)) {
        try {
          std::ifstream camera_id_file(camera_id_path);
          if (camera_id_file.is_open()) {
            std::string id;
            std::getline(camera_id_file, id);
            camera_id_file.close();
            if (!id.empty()) {
              _agent_id = id;
            }
          }
        } catch (const std::exception& e) {
          std::cout << "\033[1;33mWarning: Could not read camera_id.txt: " << e.what() << "\033[0m" << std::endl;
        }
      }

      // Get timestamp from JSON file associated with MKV
      try {
        // Look for JSON file with same name as MKV in dummy directory
        std::filesystem::path mkv_path(_params["dummy_file_path"].get<string>());
        std::filesystem::path json_path = mkv_path.parent_path() / (mkv_path.stem().string() + ".json");
        
        if (std::filesystem::exists(json_path)) {
          std::ifstream json_file(json_path);
          if (json_file.is_open()) {
            json json_data;
            json_file >> json_data;
            json_file.close();
            
            // Look for array of frame objects with timestamp_ns
            if (json_data.is_array()) {
              for (const auto& frame_obj : json_data) {
                if (frame_obj.contains("frame") && frame_obj.contains("timestamp_ns")) {
                  int frame_number = frame_obj["frame"].get<int>();
                  auto ns = frame_obj["timestamp_ns"].get<long long>();;
                  _frame_timestamps[frame_number] = chrono::steady_clock::time_point(chrono::nanoseconds(ns));
                }
              }
            }
          }
        }
      } catch (const std::exception& e) {
        cout << "\033[1;33mWarning: Could not read timestamp from JSON, proceeding without timestamp: " << e.what() << "\033[0m" << endl;
      }

      // A C++ Wrapper for the k4a_playback_t does not exist, so we use the C API directly
      if (k4a_playback_open(_params["dummy_file_path"].get<string>().c_str(), &_kinect_mkv_playback_handle) != K4A_RESULT_SUCCEEDED) {
        cout << "\033[1;31mFailed to open dummy file for playback\033[0m" << endl;
        return return_type::error;
      }
      
      // Check recording length and configuration
      uint64_t recording_length = k4a_playback_get_recording_length_usec(_kinect_mkv_playback_handle);
      if (recording_length == K4A_RESULT_SUCCEEDED) {
        cout << "Recording length: " << recording_length / 1000000.0 << " seconds" << endl;
      }
      
      if (k4a_playback_get_calibration(_kinect_mkv_playback_handle, &_kinect_calibration) != K4A_RESULT_SUCCEEDED) {
        cout << "\033[1;31mFailed to get calibration from dummy file\033[0m" << endl;
        return return_type::error;
      }

      // Get the depth camera intrinsics
      auto depth_camera_calibration = _kinect_calibration.depth_camera_calibration;

      _cx = depth_camera_calibration.intrinsics.parameters.param.cx;
      _cy = depth_camera_calibration.intrinsics.parameters.param.cy;
      _fx = depth_camera_calibration.intrinsics.parameters.param.fx;
      _fy = depth_camera_calibration.intrinsics.parameters.param.fy;
      
      // Debug section: save camera intrinsic parameters to CSV file
      if (debug) {
        // Create debug directory path relative to the source file location
        std::filesystem::path debug_dir = get_source_path();
        debug_dir = debug_dir / "Debug";
        
        // Create debug directory if it doesn't exist
        try {
          if (!std::filesystem::exists(debug_dir)) {
            std::filesystem::create_directories(debug_dir);
          }
        } catch (const std::filesystem::filesystem_error& e) {
          cout << "\033[1;33mWarning: Failed to create camera intrinsics debug directory: " << e.what() << "\033[0m" << endl;
        }
        
        string csv_filename = (debug_dir / "camera_intrinsics.csv").string();
        
        // Save camera intrinsic parameters to CSV file
        try {
          std::ofstream csv_file(csv_filename);
          if (csv_file.is_open()) {
            // Write header
            csv_file << "parameter,value\n";
            // Write intrinsic parameters
            csv_file << "cx," << std::fixed << std::setprecision(6) << _cx << "\n";
            csv_file << "cy," << std::fixed << std::setprecision(6) << _cy << "\n";
            csv_file << "fx," << std::fixed << std::setprecision(6) << _fx << "\n";
            csv_file << "fy," << std::fixed << std::setprecision(6) << _fy << "\n";
            csv_file.close();
          } else {
            cout << "\033[1;33mWarning: Failed to open camera intrinsics CSV file for writing\033[0m" << endl;
          }
        } catch (const std::exception& e) {
          cout << "\033[1;33mException saving camera intrinsics CSV: " << e.what() << "\033[0m" << endl;
        }
        
        // Create pixel-to-real-world coordinate conversion matrices for depth camera
        // Get depth camera resolution from calibration
        int depth_width = depth_camera_calibration.resolution_width;
        int depth_height = depth_camera_calibration.resolution_height;
        
        // Create X coordinate conversion matrix (pixel column to real X)
        cv::Mat x_conversion_matrix = cv::Mat::zeros(depth_height, depth_width, CV_32F);
        // Create Y coordinate conversion matrix (pixel row to real Y)  
        cv::Mat y_conversion_matrix = cv::Mat::zeros(depth_height, depth_width, CV_32F);
        
        // Fill the matrices using depth camera intrinsics
        // Formula: X_real = (u - cx) * Z / fx, Y_real = (v - cy) * Z / fy
        // Where u,v are pixel coordinates, Z is depth, cx,cy,fx,fy are intrinsics
        for (int v = 0; v < depth_height; v++) {
          for (int u = 0; u < depth_width; u++) {
            // X conversion factor: (u - cx) / fx
            x_conversion_matrix.at<float>(v, u) = (u - _cx) / _fx;
            // Y conversion factor: (v - cy) / fy
            y_conversion_matrix.at<float>(v, u) = (v - _cy) / _fy;
          }
        }
        
        // Save conversion matrices as TXT files
        std::filesystem::path matrices_debug_dir = debug_dir / "conversion matrices";
        try {
          if (!std::filesystem::exists(matrices_debug_dir)) {
            std::filesystem::create_directories(matrices_debug_dir);
          }
        } catch (const std::filesystem::filesystem_error& e) {
          cout << "\033[1;33mWarning: Failed to create conversion matrices debug directory: " << e.what() << "\033[0m" << endl;
        }
        
        string x_matrix_filename = (matrices_debug_dir / "x_conversion_matrix.txt").string();
        string y_matrix_filename = (matrices_debug_dir / "y_conversion_matrix.txt").string();
        
        // Save X conversion matrix
        try {
          std::ofstream x_file(x_matrix_filename);
          if (x_file.is_open()) {
            for (int v = 0; v < depth_height; v++) {
              for (int u = 0; u < depth_width; u++) {
                x_file << std::fixed << std::setprecision(6) << x_conversion_matrix.at<float>(v, u);
                if (u < depth_width - 1) x_file << ",";
              }
              x_file << "\n";
            }
            x_file.close();
          }
        } catch (const std::exception& e) {
          cout << "\033[1;33mException saving X conversion matrix: " << e.what() << "\033[0m" << endl;
        }
        
        // Save Y conversion matrix
        try {
          std::ofstream y_file(y_matrix_filename);
          if (y_file.is_open()) {
            for (int v = 0; v < depth_height; v++) {
              for (int u = 0; u < depth_width; u++) {
                y_file << std::fixed << std::setprecision(6) << y_conversion_matrix.at<float>(v, u);
                if (u < depth_width - 1) y_file << ",";
              }
              y_file << "\n";
            }
            y_file.close();
          }
        } catch (const std::exception& e) {
          cout << "\033[1;33mException saving Y conversion matrix: " << e.what() << "\033[0m" << endl;
        }
      }

      // Create transformation handle for color-to-depth coordinate transformation
      _mkv_color_transformation_handle = k4a_transformation_create(&_kinect_calibration);
      if (_mkv_color_transformation_handle == nullptr) {
        cout << "\033[1;31mFailed to create transformation handle\033[0m" << endl;
        return return_type::error;
      }

      _point_cloud_transformation = k4a_transformation_create(&_kinect_calibration);

      // We can use again the C++ API to create the _kinect_tracker
      _kinect_tracker_config = K4ABT_TRACKER_CONFIG_DEFAULT;
      if (_params.contains("CUDA")) {
        if (_params["CUDA"] == true) {
          cout << "Body tracker CUDA processor enabled" << endl;
          _kinect_tracker_config.processing_mode = K4ABT_TRACKER_PROCESSING_MODE_GPU_CUDA;
        } else {
          cout << "Body tracker CUDA processor disabled! Using CPU instead." << endl;
          _kinect_tracker_config.processing_mode = K4ABT_TRACKER_PROCESSING_MODE_CPU;
        }
      }
      _kinect_tracker = k4abt::tracker::create(_kinect_calibration, _kinect_tracker_config);

      cout << "Attempting to get first capture from MKV file..." << endl;
      if (k4a_playback_get_next_capture(_kinect_mkv_playback_handle, &_kinect_mkv_capture_handle) != K4A_RESULT_SUCCEEDED) {
        cout << "\033[1;31mFailed to get next capture from dummy file during setup\033[0m" << endl;
        cout << "\033[1;33mThis might indicate the MKV file is corrupted or has no data\033[0m" << endl;
        return return_type::error;
      }

      // Wrap the C handle in a C++ object
      _k4a_rgbd_capture = k4a::capture(_kinect_mkv_capture_handle);
      
      // Check if the capture is valid
      if (!_k4a_rgbd_capture) {
        cout << "\033[1;31mInvalid capture from dummy file during setup\033[0m" << endl;
        return return_type::error;
      }
      
      _k4a_color_image = _k4a_rgbd_capture.get_color_image();
      _k4a_depth_image = _k4a_rgbd_capture.get_depth_image();

      while (!_k4a_color_image.is_valid())
      {
        _global_frame_counter++;
        cout << "\033[1;33mThis frame has no valid color image, trying next frame...\033[0m" << endl;
        if (k4a_playback_get_next_capture(_kinect_mkv_playback_handle, &_kinect_mkv_capture_handle) != K4A_RESULT_SUCCEEDED) {
          cout << "\033[1;31mFailed to get second capture from dummy file during setup\033[0m" << endl;
          return return_type::error;
        }

        _k4a_rgbd_capture = k4a::capture(_kinect_mkv_capture_handle);
        _k4a_color_image = _k4a_rgbd_capture.get_color_image();
        _k4a_depth_image = _k4a_rgbd_capture.get_depth_image();

        cout << "During setup - Color image valid: " << (_k4a_color_image.is_valid() ? "Yes" : "No") << endl;
        cout << "During setup - Depth image valid: " << (_k4a_depth_image.is_valid() ? "Yes" : "No") << endl;
      }

      _k4a_color_image = transform_color_to_depth_coordinates(_mkv_color_transformation_handle, _k4a_color_image, _k4a_depth_image);

      //TODO: è da fare nel distruttore
      // Clean up the transformation handle
      //k4a_transformation_destroy(_mkv_color_transformation_handle);

      // Convert k4a::image to cv::Mat --> color image
      if (k4a_color_image_to_cv_mat(_k4a_color_image, _rgb) != return_type::success) {
        cout << "\033[1;31mFailed to convert k4a::image to cv::Mat!\033[0m" << endl;
        return return_type::error;
      }
      
      // Save the RGB image for debugging
      if (debug) {
        string filename = "mkv_rgb_transformed_to_depth_coordinates.jpg";
        
        // Create debug directory path relative to the source file location
        std::filesystem::path debug_dir;
        
        // Get the directory of the current source file
        std::filesystem::path source_file_path = std::filesystem::path(__FILE__);
        std::filesystem::path source_dir = source_file_path.parent_path();
        std::filesystem::path hpe_path;
        
        // Look for hpe directory starting from source file directory and going up
        std::filesystem::path search_path = source_dir;
        bool found_hpe = false;
        
        for (int i = 0; i < 5; ++i) { // Search up to 5 levels up
          std::filesystem::path potential_hpe = search_path / "hpe";
          if (std::filesystem::exists(potential_hpe) && std::filesystem::is_directory(potential_hpe)) {
            hpe_path = potential_hpe;
            found_hpe = true;
            break;
          }
          // If we're already in a directory named "hpe", use it
          if (search_path.filename() == "hpe") {
            hpe_path = search_path;
            found_hpe = true;
            break;
          }
          if (search_path.has_parent_path()) {
            search_path = search_path.parent_path();
          } else {
            break;
          }
        }
        
        if (found_hpe) {
          debug_dir = hpe_path / "Debug";
        } else {
          // Fallback: use source directory + Debug
          debug_dir = source_dir / "Debug";
        }
        
        // Create debug directory if it doesn't exist
        try {
          if (!std::filesystem::exists(debug_dir)) {
            std::filesystem::create_directories(debug_dir);
            cout << "Created debug directory: " << debug_dir << endl;
          }
        } catch (const std::filesystem::filesystem_error& e) {
          cout << "\033[1;33mWarning: Failed to create debug directory: " << e.what() << "\033[0m" << endl;
          debug_dir = source_dir; // Use source directory as fallback
        }
        
        string output_filename = (debug_dir / filename).string();
        
        try {
          bool saved = cv::imwrite(output_filename, _rgb);
          if (saved) {
            cout << "\033[1;32mTransformed RGB image saved as: " << output_filename << "\033[0m" << endl;
          } else {
            cout << "\033[1;33mWarning: Failed to save transformed RGB image\033[0m" << endl;
          }
        } catch (const cv::Exception& e) {
          cout << "\033[1;33mException saving transformed RGB image: " << e.what() << "\033[0m" << endl;
        }
      }
      #endif
    }
    else if (_video_source == RGB_CAMERA_DUMMY || _video_source == RASPI_RGB_CAMERA_DUMMY){

    }
    else {
      cout << "Unknown video source type. Cannot acquire frame." << endl;
      return return_type::error;
    }

    if (_video_source != KINECT_AZURE_DUMMY && _video_source != RGB_CAMERA_DUMMY && _video_source != RASPI_RGB_CAMERA_DUMMY)
      _frame_time = chrono::steady_clock::now();
    else{
      _frame_time = _frame_timestamps[_global_frame_counter];
      cout << "Frame time: " << chrono::duration_cast<chrono::nanoseconds>(_frame_time.time_since_epoch()).count() << " ns" << endl;
      cout << "Frame counter: " << _global_frame_counter << endl;
    }
    _global_frame_counter++;

    cv::Size resolution = _rgb.size();

    // If KINECT_AZURE don't change the resolution because the rgb image 
    //is in the coordinates of the depth image and MUST stay that way
    if (_video_source != KINECT_AZURE_DUMMY && _video_source != KINECT_AZURE_CAMERA){
      if(found != string::npos){
        resolution = cv::Size(_rgb_width_read, _rgb_height_read);

        _output_transform = OutputTransform(_rgb.size(), resolution);
        resolution = _output_transform.computeResolution();
        cv::resize(_rgb, _rgb, cv::Size(resolution.width, resolution.height));
      }
    }

    _rgb_height = resolution.height;
    _rgb_width = resolution.width;

    

    return return_type::success;
  }

  /* Setup camera extrinsics from parameters
  */
  return_type setup_camera_extrinsics(bool calibration = false, bool debug = false) {
    // Setup camera extrinsics if provided in parameters
    
    // Initialize transformation matrix as identity
    _camera_transformation_matrix = Eigen::Matrix4f::Identity();
    
    // Check if camera serial number is provided in params
    if (debug) {
      cout << "\033[1;34mSetup camera extrinsics for camera serial: SN" << _agent_id << "\033[0m" << endl;
    }
    
    // Check if transformation parameters exist for this camera serial
    if (!calibration && _params.contains("SN" + _agent_id)) {
      auto camera_params = _params["SN" + _agent_id];
      
      // Read rotation matrix components
      float Rxx = camera_params.contains("Rxx") ? camera_params["Rxx"].get<float>() : 1.0f;
      float Rxy = camera_params.contains("Rxy") ? camera_params["Rxy"].get<float>() : 0.0f;
      float Rxz = camera_params.contains("Rxz") ? camera_params["Rxz"].get<float>() : 0.0f;
      float Ryx = camera_params.contains("Ryx") ? camera_params["Ryx"].get<float>() : 0.0f;
      float Ryy = camera_params.contains("Ryy") ? camera_params["Ryy"].get<float>() : 1.0f;
      float Ryz = camera_params.contains("Ryz") ? camera_params["Ryz"].get<float>() : 0.0f;
      float Rzx = camera_params.contains("Rzx") ? camera_params["Rzx"].get<float>() : 0.0f;
      float Rzy = camera_params.contains("Rzy") ? camera_params["Rzy"].get<float>() : 0.0f;
      float Rzz = camera_params.contains("Rzz") ? camera_params["Rzz"].get<float>() : 1.0f;
      
      // Read translation vector components
      float Tx = camera_params.contains("Tx") ? camera_params["Tx"].get<float>() : 0.0f;
      float Ty = camera_params.contains("Ty") ? camera_params["Ty"].get<float>() : 0.0f;
      float Tz = camera_params.contains("Tz") ? camera_params["Tz"].get<float>() : 0.0f;
      
      // Build the 4x4 transformation matrix
      // [Rxx Rxy Rxz Tx]
      // [Ryx Ryy Ryz Ty]
      // [Rzx Rzy Rzz Tz]
      // [ 0   0   0   1]
      _camera_transformation_matrix << Rxx, Rxy, Rxz, Tx,
                                      Ryx, Ryy, Ryz, Ty,
                                      Rzx, Rzy, Rzz, Tz,
                                      0.0f, 0.0f, 0.0f, 1.0f;
      
      if (debug) {
        cout << "\033[1;32mLoaded camera transformation matrix for SN" << _agent_id << "\033[0m" << endl;
        cout << _camera_transformation_matrix << endl;
      }
    } else {

        if (calibration)
          cout << "\033[1;33mCalibration mode active, using the identity matrix\033[0m" << endl;
        else
          cout << "\033[1;33mWarning: No transformation parameters found for camera serial SN" << _agent_id << ", using identity matrix\033[0m" << endl;

      return return_type::warning;
    }

    return return_type::success;
  }

  /**
   * Apply camera transformation matrix to a 3D point
   * @param point The input 3D point (x, y, z)
   * @return The transformed 3D point
   */
  Eigen::Vector3f apply_camera_transformation(const Eigen::Vector3f& point) {
    // Convert 3D point to homogeneous coordinates
    Eigen::Vector4f homogeneous_point(point.x(), point.y(), point.z(), 1.0f);
    
    // Apply transformation matrix
    Eigen::Vector4f transformed_homogeneous = _camera_transformation_matrix * homogeneous_point;
    
    // Convert back to 3D coordinates
    return Eigen::Vector3f(transformed_homogeneous.x(), transformed_homogeneous.y(), transformed_homogeneous.z());
  }

  /**
   * Apply camera transformation matrix to a 3D point represented as vector<float>
   * @param point The input 3D point as {x,y,z}
   * @return The transformed 3D point as {x,y,z}
   */
  std::vector<float> apply_camera_transformation(const std::vector<float> point) {
    if (point.size() < 3) {
      // Not enough components, return empty vector
      return std::vector<float>();
    }
    Eigen::Vector3f eigen_point(static_cast<float>(point[0]),
                                static_cast<float>(point[1]),
                                static_cast<float>(point[2]));
    Eigen::Vector3f transformed = apply_camera_transformation(eigen_point);
    return std::vector<float>{transformed.x(), transformed.y(), transformed.z()};
  }

  /**
   * Apply camera transformation matrix to a 3D matrix
   * @param matrix The input 3D matrix
   * @return The transformed 3D matrix
   */
  Eigen::Matrix3f apply_camera_transformation(const Eigen::Matrix3f& matrix) {

    // Apply transformation matrix
    Eigen::Matrix3f transformed = _camera_transformation_matrix.block<3, 3>(0, 0).transpose() * matrix * _camera_transformation_matrix.block<3, 3>(0, 0);
    return transformed;
  }

  #ifdef KINECT_AZURE_LIBS
  k4a::image transform_color_to_depth_coordinates(k4a_transformation_t transformation_handle, 
                                                  k4a::image color_image, 
                                                  k4a::image depth_image) {
    if (!depth_image.is_valid()) {
      cout << "\033[1;33mWarning: No depth image available, skipping color-to-depth transformation\033[0m" << endl;
      return color_image;
    }
    
    if (!color_image.is_valid()) {
      cout << "\033[1;31mError: Invalid color image provided for transformation\033[0m" << endl;
      return color_image;
    }
    
    // Check if the color image is compressed (JPEG) or raw (BGRA32)
    k4a_image_format_t color_format = color_image.get_format();
    
    k4a::image color_image_for_transform;
    
    if (color_format == K4A_IMAGE_FORMAT_COLOR_BGRA32) {
      color_image_for_transform = color_image;
    } else {
      // Decompress the image to get raw BGRA32 data
      size_t buffer_size = color_image.get_size();
      uint8_t *compressed_buffer = color_image.get_buffer();
      
      // Create a temporary Mat from the compressed buffer
      vector<uint8_t> compressed_data(compressed_buffer, compressed_buffer + buffer_size);
      
      // Decode the compressed image
      cv::Mat decoded_image = cv::imdecode(compressed_data, cv::IMREAD_COLOR);
      
      if (decoded_image.empty()) {
        cout << "\033[1;31mFailed to decode compressed image for transformation!\033[0m" << endl;
        return color_image; // Use original as fallback
      }
      
      // Convert BGR to BGRA
      cv::Mat bgra_image;
      cv::cvtColor(decoded_image, bgra_image, cv::COLOR_BGR2BGRA);
      
      // Create a new k4a::image with the decompressed BGRA data
      color_image_for_transform = k4a::image::create(
        K4A_IMAGE_FORMAT_COLOR_BGRA32,
        bgra_image.cols,
        bgra_image.rows,
        bgra_image.cols * 4 * sizeof(uint8_t)
      );
      
      if (color_image_for_transform.is_valid()) {
        // Copy the BGRA data to the k4a::image
        uint8_t *k4a_buffer = color_image_for_transform.get_buffer();
        memcpy(k4a_buffer, bgra_image.data, bgra_image.total() * bgra_image.elemSize());
      } else {
        cout << "\033[1;33mWarning: Failed to create BGRA32 k4a::image, using original\033[0m" << endl;
        return color_image;
      }
    }
    
    // Now proceed with the transformation using the BGRA32 image
    if (color_image_for_transform.is_valid() && color_image_for_transform.get_format() == K4A_IMAGE_FORMAT_COLOR_BGRA32) {
      // Create a new k4a::image for the transformed color image
      k4a::image transformed_color_image = k4a::image::create(
        K4A_IMAGE_FORMAT_COLOR_BGRA32,
        depth_image.get_width_pixels(),
        depth_image.get_height_pixels(),
        depth_image.get_width_pixels() * 4 * sizeof(uint8_t)
      );
      
      if (transformed_color_image.is_valid()) {
        // Transform color image to depth camera coordinates
        k4a_result_t transform_result = k4a_transformation_color_image_to_depth_camera(
          transformation_handle,
          depth_image.handle(),
          color_image_for_transform.handle(),
          transformed_color_image.handle()
        );
        
        if (transform_result == K4A_RESULT_SUCCEEDED) {
          return transformed_color_image;
        } else {
          cout << "\033[1;33mWarning: Color to depth transformation failed, using original color image\033[0m" << endl;
        }
      } else {
        cout << "\033[1;33mWarning: Failed to create transformed color image buffer\033[0m" << endl;
      }
    } else {
      cout << "\033[1;33mWarning: Cannot transform - color image is not in BGRA32 format\033[0m" << endl;
    }
    
    return color_image; // Return original if transformation failed
  }
  #endif

  #ifdef KINECT_AZURE_LIBS
  return_type k4a_color_image_to_cv_mat(k4a::image k4a_image, cv::Mat& output_mat) {
    
    if (!k4a_image.is_valid()) {
      cout << "\033[1;31mInvalid k4a::image provided\033[0m" << endl;
      return return_type::error;
    }
    
    int rows = k4a_image.get_height_pixels();
    int cols = k4a_image.get_width_pixels();
    
    // Check buffer size
    size_t buffer_size = k4a_image.get_size();
    uint8_t *buffer = k4a_image.get_buffer();
    
    // Calculate expected buffer size
    size_t expected_size = rows * cols * 4; // 4 bytes per pixel for BGRA
    
    // Get the stride (bytes per row) from the image
    size_t stride = k4a_image.get_stride_bytes();
    
    // Check image format
    k4a_image_format_t format = k4a_image.get_format();
    
    if (stride == 0 || buffer_size < expected_size) {
      // Handle compressed image
      vector<uint8_t> compressed_data(buffer, buffer + buffer_size);
      
      // Decode the compressed image
      cv::Mat decoded_image = cv::imdecode(compressed_data, cv::IMREAD_COLOR);
      
      if (decoded_image.empty()) {
        cout << "\033[1;31mFailed to decode compressed image!\033[0m" << endl;
        return return_type::error;
      }

      // Convert to BGR if necessary (OpenCV imdecode usually returns BGR)
      if (decoded_image.channels() == 3) {
        output_mat = decoded_image.clone();
      } else {
        cv::cvtColor(decoded_image, output_mat, cv::COLOR_BGRA2BGR);
      }
    } else {
      // Handle raw BGRA data
      try {
        cv::Mat temp_mat = cv::Mat(rows, cols, CV_8UC4, (void *)buffer, stride);
        cv::cvtColor(temp_mat, output_mat, cv::COLOR_BGRA2BGR);
      } catch (const cv::Exception& e) {
        cout << "\033[1;31mColor conversion failed: " << e.what() << "\033[0m" << endl;
        return return_type::error;
      } catch (const std::exception& e) {
        cout << "\033[1;31mException creating Mat: " << e.what() << "\033[0m" << endl;
        return return_type::error;
      }
    }
    
    // Final check if Mat was created successfully
    if (output_mat.empty()) {
      cout << "\033[1;31mFailed to create cv::Mat from k4a::image!\033[0m" << endl;
      return return_type::error;
    }  

    return return_type::success;
  }
  #endif

  #ifdef KINECT_AZURE_LIBS
  return_type k4a_depth_image_to_cv_mat(k4a::image k4a_depth_image, cv::Mat& output_mat) {
    
    if (!k4a_depth_image.is_valid()) {
      cout << "\033[1;31mInvalid k4a depth image provided\033[0m" << endl;
      return return_type::error;
    }
    
    int rows = k4a_depth_image.get_height_pixels();
    int cols = k4a_depth_image.get_width_pixels();
    
    // Check buffer size
    size_t buffer_size = k4a_depth_image.get_size();
    uint8_t *buffer = k4a_depth_image.get_buffer();
    
    // Calculate expected buffer size for raw depth image (2 bytes per pixel)
    size_t expected_raw_size = rows * cols * 2; // 2 bytes per pixel for 16-bit depth
    
    // Get the stride (bytes per row) from the image
    size_t stride = k4a_depth_image.get_stride_bytes();
    
    // Check image format
    k4a_image_format_t format = k4a_depth_image.get_format();
    
    if (format != K4A_IMAGE_FORMAT_DEPTH16) {
      cout << "\033[1;31mUnsupported depth image format: " << format << "\033[0m" << endl;
      return return_type::error;
    }
    
    // Check if the depth image is compressed (from MKV file)
    if (stride == 0 || buffer_size < expected_raw_size) {
      // Try to decompress using OpenCV (some MKV files use standard compression)
      vector<uint8_t> compressed_data(buffer, buffer + buffer_size);
      cv::Mat decoded_image = cv::imdecode(compressed_data, cv::IMREAD_ANYDEPTH | cv::IMREAD_ANYCOLOR);
      
      if (!decoded_image.empty()) {
        // Convert to 16-bit if necessary
        if (decoded_image.type() == CV_16U) {
          output_mat = decoded_image.clone();
        } else {
          decoded_image.convertTo(output_mat, CV_16U);
        }
      } else {
        cout << "\033[1;33mWarning: Failed to decode compressed depth data with OpenCV\033[0m" << endl;
        cout << "\033[1;33mAssuming raw depth data despite size mismatch\033[0m" << endl;
        
        // Fallback: try to interpret as raw data anyway
        if (stride == 0) {
          stride = cols * 2; // Default stride for 16-bit depth
        }
        
        try {
          cv::Mat temp_mat = cv::Mat(rows, cols, CV_16U, (void *)buffer, stride);
          output_mat = temp_mat.clone();
        } catch (const cv::Exception& e) {
          cout << "\033[1;31mFailed to create Mat from raw depth data: " << e.what() << "\033[0m" << endl;
          return return_type::error;
        }
      }
    } else {
      // Handle raw depth data (16-bit unsigned integers)
      
      try {
        if (stride == 0) {
          stride = cols * 2; // Default stride for 16-bit depth
        }
        
        // Create Mat from raw depth data
        cv::Mat temp_mat = cv::Mat(rows, cols, CV_16U, (void *)buffer, stride);
        
        // Clone the data to ensure it's owned by output_mat
        output_mat = temp_mat.clone();
        
      } catch (const cv::Exception& e) {
        cout << "\033[1;31mDepth image conversion failed: " << e.what() << "\033[0m" << endl;
        return return_type::error;
      } catch (const std::exception& e) {
        cout << "\033[1;31mException creating depth Mat: " << e.what() << "\033[0m" << endl;
        return return_type::error;
      }
    }
    
    // Final check if Mat was created successfully
    if (output_mat.empty()) {
      cout << "\033[1;31mFailed to create cv::Mat from k4a depth image!\033[0m" << endl;
      return return_type::error;
    }

    return return_type::success;
  }
  #endif

  #ifdef KINECT_AZURE_LIBS
  void mask_depth_with_body_index(const k4a::image &depth_image,
                                  const k4a::image &body_index_map,
                                  k4a::image &masked_depth_image) {
    // Get image dimensions
    int width = depth_image.get_width_pixels();
    int height = depth_image.get_height_pixels();

    // Get pointers to the image data
    const uint16_t *depth_data =
        reinterpret_cast<const uint16_t *>(depth_image.get_buffer());
    const uint8_t *body_index_data =
        reinterpret_cast<const uint8_t *>(body_index_map.get_buffer());
    uint16_t *masked_depth_data =
        reinterpret_cast<uint16_t *>(masked_depth_image.get_buffer());

    // Iterate over each pixel
    for (int y = 0; y < height; y++) {
      for (int x = 0; x < width; x++) {
        int idx = y * width + x;

        // Check if the pixel belongs to a body
        if (body_index_data[idx] != K4ABT_BODY_INDEX_MAP_BACKGROUND) {
          // Copy the depth value
          masked_depth_data[idx] = depth_data[idx];
        } else {
          // Set to zero or any background value (e.g., 0 for no depth)
          masked_depth_data[idx] = 0;
        }
      }
    }
  }
  #endif

  #ifdef KINECT_AZURE_LIBS
  return_type create_point_cloud(k4a_transformation_t transformation_handle,
                                   const k4a_image_t depth_image,
                                   const k4a_image_t color_image) {
    // Check if the color image is compressed (JPEG) or raw (BGRA32)
    k4a_image_format_t color_format = k4a_image_get_format(color_image);
    
    k4a_image_t color_image_for_transform = NULL;
    
    if (color_format == K4A_IMAGE_FORMAT_COLOR_BGRA32) {
      // Use the original image directly
      color_image_for_transform = const_cast<k4a_image_t>(color_image);
      k4a_image_reference(color_image_for_transform);
    } else {
      // Decompress the image to get raw BGRA32 data
      size_t buffer_size = k4a_image_get_size(color_image);
      uint8_t *compressed_buffer = k4a_image_get_buffer(color_image);
      
      // Create a temporary Mat from the compressed buffer
      vector<uint8_t> compressed_data(compressed_buffer, compressed_buffer + buffer_size);
      
      // Decode the compressed image
      cv::Mat decoded_image = cv::imdecode(compressed_data, cv::IMREAD_COLOR);
      
      if (decoded_image.empty()) {
        cout << "\033[1;31mFailed to decode compressed image for point cloud!\033[0m" << endl;
        return return_type::error;
      }
      
      // Convert BGR to BGRA
      cv::Mat bgra_image;
      cv::cvtColor(decoded_image, bgra_image, cv::COLOR_BGR2BGRA);
      
      // Create a new k4a_image_t with the decompressed BGRA data
      if (K4A_RESULT_SUCCEEDED != k4a_image_create(
          K4A_IMAGE_FORMAT_COLOR_BGRA32,
          bgra_image.cols,
          bgra_image.rows,
          bgra_image.cols * 4 * sizeof(uint8_t),
          &color_image_for_transform)) {
        cout << "\033[1;31mFailed to create BGRA32 k4a_image_t for point cloud\033[0m" << endl;
        return return_type::error;
      }
      
      // Copy the BGRA data to the k4a_image_t
      uint8_t *k4a_buffer = k4a_image_get_buffer(color_image_for_transform);
      memcpy(k4a_buffer, bgra_image.data, bgra_image.total() * bgra_image.elemSize());
    }
    
    int depth_image_width_pixels = k4a_image_get_width_pixels(depth_image);
    int depth_image_height_pixels = k4a_image_get_height_pixels(depth_image);
    k4a_image_t transformed_color_image = NULL;
    if (K4A_RESULT_SUCCEEDED !=
        k4a_image_create(K4A_IMAGE_FORMAT_COLOR_BGRA32,
                          depth_image_width_pixels, depth_image_height_pixels,
                          depth_image_width_pixels * 4 * (int)sizeof(uint8_t),
                          &transformed_color_image)) {
      printf("Failed to create transformed color image\n");
      return return_type::error;
    }

    k4a_image_t point_cloud_image = NULL;
    if (K4A_RESULT_SUCCEEDED !=
        k4a_image_create(K4A_IMAGE_FORMAT_CUSTOM, depth_image_width_pixels,
                          depth_image_height_pixels,
                          depth_image_width_pixels * 3 * (int)sizeof(int16_t),
                          &point_cloud_image)) {
      printf("Failed to create point cloud image\n");
      return return_type::error;
    }

    if (K4A_RESULT_SUCCEEDED !=
        k4a_transformation_color_image_to_depth_camera(
            transformation_handle, depth_image, color_image_for_transform,
            transformed_color_image)) {
      printf("Failed to compute transformed color image\n");
      // Clean up the color image if it was created
      if (color_format != K4A_IMAGE_FORMAT_COLOR_BGRA32) {
        k4a_image_release(color_image_for_transform);
      }
      return return_type::error;
    }

    if (K4A_RESULT_SUCCEEDED != k4a_transformation_depth_image_to_point_cloud(
                                    transformation_handle, depth_image,
                                    K4A_CALIBRATION_TYPE_DEPTH,
                                    point_cloud_image)) {
      printf("Failed to compute point cloud\n");
      // Clean up the color image if it was created
      if (color_format != K4A_IMAGE_FORMAT_COLOR_BGRA32) {
        k4a_image_release(color_image_for_transform);
      }
      return return_type::error;
    }

    std::vector<color_point_t> points;

    int width = k4a_image_get_width_pixels(point_cloud_image);
    int height = k4a_image_get_height_pixels(transformed_color_image);

    int16_t *point_cloud_image_data =
        (int16_t *)(void *)k4a_image_get_buffer(point_cloud_image);
    uint8_t *color_image_data = k4a_image_get_buffer(transformed_color_image);

    for (int i = 0; i < width * height; i++) {
      color_point_t point;
      point.xyz[0] = point_cloud_image_data[3 * i + 0];
      point.xyz[1] = point_cloud_image_data[3 * i + 1];
      point.xyz[2] = point_cloud_image_data[3 * i + 2];
      if (point.xyz[2] == 0) {
        continue;
      }

      point.rgb[0] = color_image_data[4 * i + 0];
      point.rgb[1] = color_image_data[4 * i + 1];
      point.rgb[2] = color_image_data[4 * i + 2];
      uint8_t alpha = color_image_data[4 * i + 3];

      if (point.rgb[0] == 0 && point.rgb[1] == 0 && point.rgb[2] == 0 &&
          alpha == 0) {
        continue;
      }

      points.push_back(point);
    }

    // convert the points to a point cloud of Mat type
    _point_cloud = cv::Mat(points.size(), 6, CV_32F);
    for (size_t i = 0; i < points.size(); i++) {
      _point_cloud.at<float>(i, 0) = points[i].xyz[0];
      _point_cloud.at<float>(i, 1) = points[i].xyz[1];
      _point_cloud.at<float>(i, 2) = points[i].xyz[2];
      _point_cloud.at<float>(i, 3) = points[i].rgb[2];
      _point_cloud.at<float>(i, 4) = points[i].rgb[1];
      _point_cloud.at<float>(i, 5) = points[i].rgb[0];
    }

    // Save the point cloud to a ply file
    // write_ply_from_points_vector(points,
    // "../plugin_skeletonizer_3D/test_points.ply");
    // write_ply_from_cv_mat(_point_cloud,
    // "../plugin_skeletonizer_3D/test_cv_mat.ply");

    // Clean up the color image if it was created for decompression
    if (color_format != K4A_IMAGE_FORMAT_COLOR_BGRA32) {
      k4a_image_release(color_image_for_transform);
    }

    return return_type::success;
  }
  #endif

  void write_ply_from_cv_mat(cv::Mat point_cloud, const char *file_name) {
    #define PLY_START_HEADER "ply"
    #define PLY_END_HEADER "end_header"
    #define PLY_ASCII "format ascii 1.0"
    #define PLY_ELEMENT_VERTEX "element vertex"

    // save to the ply file
    std::ofstream ofs(file_name); // text mode first
    ofs << PLY_START_HEADER << std::endl;
    ofs << PLY_ASCII << std::endl;
    ofs << PLY_ELEMENT_VERTEX << " " << point_cloud.rows << std::endl;
    ofs << "property float x" << std::endl;
    ofs << "property float y" << std::endl;
    ofs << "property float z" << std::endl;
    ofs << "property uchar red" << std::endl;
    ofs << "property uchar green" << std::endl;
    ofs << "property uchar blue" << std::endl;
    ofs << PLY_END_HEADER << std::endl;
    ofs.close();

    std::stringstream ss;
    for (int i = 0; i < point_cloud.rows; ++i) {
      ss << point_cloud.at<float>(i, 0) << " " << point_cloud.at<float>(i, 1)
          << " " << point_cloud.at<float>(i, 2);
      ss << " " << point_cloud.at<float>(i, 3) << " "
          << point_cloud.at<float>(i, 4) << " " << point_cloud.at<float>(i, 5);
      ss << std::endl;
    }
    std::ofstream ofs_text(file_name, std::ios::out | std::ios::app);
    ofs_text.write(ss.str().c_str(), (std::streamsize)ss.str().length());
  }

  static void write_ply_from_points_vector(std::vector<color_point_t> points,
                                             const char *file_name) {

    #define PLY_START_HEADER "ply"
    #define PLY_END_HEADER "end_header"
    #define PLY_ASCII "format ascii 1.0"
    #define PLY_ELEMENT_VERTEX "element vertex"

    // save to the ply file
    std::ofstream ofs(file_name); // text mode first
    ofs << PLY_START_HEADER << std::endl;
    ofs << PLY_ASCII << std::endl;
    ofs << PLY_ELEMENT_VERTEX << " " << points.size() << std::endl;
    ofs << "property float x" << std::endl;
    ofs << "property float y" << std::endl;
    ofs << "property float z" << std::endl;
    ofs << "property uchar red" << std::endl;
    ofs << "property uchar green" << std::endl;
    ofs << "property uchar blue" << std::endl;
    ofs << PLY_END_HEADER << std::endl;
    ofs.close();

    std::stringstream ss;
    for (size_t i = 0; i < points.size(); ++i) {
      // image data is BGR
      ss << (float)points[i].xyz[0] << " " << (float)points[i].xyz[1] << " "
          << (float)points[i].xyz[2];
      ss << " " << (float)points[i].rgb[2] << " " << (float)points[i].rgb[1]
          << " " << (float)points[i].rgb[0];
      ss << std::endl;
    }
    std::ofstream ofs_text(file_name, std::ios::out | std::ios::app);
    ofs_text.write(ss.str().c_str(), (std::streamsize)ss.str().length());
  }

  /**
   * @brief Acquire a frame from a camera device. Camera ID is defined in the
   * parameters list.
   *
   * The acquired frame is stored in the #_k4a_rgbd, #_rgbd and #_rgb
   * attributes.
   *
   * @see set_params
   * @author Nicola
   * @return result status ad defined in return_type
   */
  return_type acquire_frame( bool debug = false) {

    _frame_time = chrono::steady_clock::now();

    if (_video_source == KINECT_AZURE_CAMERA){
      #ifdef KINECT_AZURE_LIBS

      _kinect_device.get_capture(&_k4a_rgbd_capture, std::chrono::milliseconds(K4A_WAIT_INFINITE));

      _k4a_color_image = _k4a_rgbd_capture.get_color_image();
      
      // Validate the color image before transformation
      if (!_k4a_color_image.is_valid()) {
        cout << "\033[1;31mError: Failed to get valid color image from capture in acquire_frame\033[0m" << endl;
        return return_type::error;
      }

      // Transform the color image into depth image coordinates before converting into cv::Mat
      _k4a_depth_image = _k4a_rgbd_capture.get_depth_image();
      
      // Validate the depth image before transformation
      if (!_k4a_depth_image.is_valid()) {
        cout << "\033[1;31mError: Failed to get valid depth image from capture in acquire_frame\033[0m" << endl;
        return return_type::error;
      }
      
      _k4a_color_image = transform_color_to_depth_coordinates(_kinect_color_transformation_handle, _k4a_color_image, _k4a_depth_image);

      // Convert k4a::image to cv::Mat --> color image
      if (k4a_color_image_to_cv_mat(_k4a_color_image, _rgb) != return_type::success) {
        cout << "\033[1;31mFailed to convert k4a::image to cv::Mat!\033[0m" << endl;
        return return_type::error;
      }

      // Convert k4a::image to cv::Mat --> depth image
      if (k4a_depth_image_to_cv_mat(_k4a_depth_image, _rgbd) != return_type::success) {
        cout << "\033[1;31mFailed to convert k4a::image to cv::Mat!\033[0m" << endl;
        return return_type::error;
      }
    
      #endif
    }
    else if (_video_source == RGB_CAMERA){
      _cap >> _rgb;
    }
    else if (_video_source == RASPI_RGB_CAMERA){
      #ifdef __linux
      _raspi_rgb_camera.getVideoFrame(_rgb, 100);
      #endif
    }
    else if (_video_source == KINECT_AZURE_DUMMY) {
      #ifdef KINECT_AZURE_LIBS

      // get time of the current frame
      _frame_time = _frame_timestamps[_global_frame_counter];
      //cout << "Frame time: " << chrono::duration_cast<chrono::nanoseconds>(_frame_time.time_since_epoch()).count() << " ns" << endl;
      //cout << "Frame counter: " << _global_frame_counter << endl;
      
      // Increment global frame counter after each frame processing (success or failure)
      _global_frame_counter++;

      k4a_stream_result_t stream_result = k4a_playback_get_next_capture(_kinect_mkv_playback_handle, &_kinect_mkv_capture_handle);
      if (stream_result == K4A_STREAM_RESULT_EOF) {
        cout << "\033[1;31mEnd of MKV file reached. Terminating plugin execution.\033[0m" << endl;
        // Clean up resources before terminating
        if (_kinect_mkv_playback_handle) {
          k4a_playback_close(_kinect_mkv_playback_handle);
          _kinect_mkv_playback_handle = nullptr;
        }
        exit(0); // Terminate the process
      } else if (stream_result != K4A_STREAM_RESULT_SUCCEEDED) {
        cout << "\033[1;31mFailed to get next capture from dummy file\033[0m" << endl;
        return return_type::error;
      }

      // Wrap the C handle in a C++ object
      _k4a_rgbd_capture = k4a::capture(_kinect_mkv_capture_handle);
      
      // Check if the capture is valid
      if (!_k4a_rgbd_capture) {
        cout << "\033[1;31mInvalid capture from dummy file\033[0m" << endl;
        return return_type::error;
      }
      
      _k4a_color_image = _k4a_rgbd_capture.get_color_image();
      _k4a_depth_image = _k4a_rgbd_capture.get_depth_image();
      
      //cout << "Color image valid: " << (_k4a_color_image.is_valid() ? "Yes" : "No") << endl;
      //cout << "Depth image valid: " << (_k4a_depth_image.is_valid() ? "Yes" : "No") << endl;
      
      // Validate the color image before transformation
      if (!_k4a_color_image.is_valid()) {
        cout << "\033[1;31mError: Failed to get valid color image from MKV file in acquire_frame\033[0m" << endl;
        cout << "\033[1;33mThis MKV file might not contain color stream data\033[0m" << endl;
        return return_type::error;
      }
      
      // Validate the depth image before transformation
      if (!_k4a_depth_image.is_valid()) {
        cout << "\033[1;31mError: Failed to get valid depth image from MKV file in acquire_frame\033[0m" << endl;
        cout << "\033[1;33mThis MKV file might not contain depth stream data\033[0m" << endl;
        return return_type::error;
      }
      
      _k4a_color_image = transform_color_to_depth_coordinates(_mkv_color_transformation_handle, _k4a_color_image, _k4a_depth_image);

      // Convert k4a::image to cv::Mat --> color image
      if (k4a_color_image_to_cv_mat(_k4a_color_image, _rgb) != return_type::success) {
        cout << "\033[1;31mFailed to convert k4a::image to cv::Mat!\033[0m" << endl;
        return return_type::error;
      }

      // Convert k4a::image to cv::Mat --> depth image
      if (k4a_depth_image_to_cv_mat(_k4a_depth_image, _rgbd) != return_type::success) {
        cout << "\033[1;31mFailed to convert k4a::image to cv::Mat!\033[0m" << endl;
        return return_type::error;
      }
      
      // Debug section: save RGB and depth images
      if (debug) {
        // Create debug directory paths relative to the source file location
        std::filesystem::path debug_dir = get_source_path();
        std::filesystem::path rgb_debug_dir = debug_dir / "Debug" / "rgb images in depth coordinates";
        std::filesystem::path depth_debug_dir = debug_dir / "Debug" / "depth images";
        
        // Create debug directories if they don't exist
        try {
          if (!std::filesystem::exists(rgb_debug_dir)) {
            std::filesystem::create_directories(rgb_debug_dir);
          }
          if (!std::filesystem::exists(depth_debug_dir)) {
            std::filesystem::create_directories(depth_debug_dir);
          }
        } catch (const std::filesystem::filesystem_error& e) {
          cout << "\033[1;33mWarning: Failed to create image debug directories: " << e.what() << "\033[0m" << endl;
        }

        // Get timestamp from JSON file associated with MKV loaded in setup_video_capture
        string timestamp = "";
        bool has_timestamp = false;
        try {
          timestamp = std::to_string(_frame_time.time_since_epoch().count());
          has_timestamp = true;
        }
        catch (const std::out_of_range& e) {
          cout << "\033[1;33mWarning: Could not read timestamp from MKV, proceeding without timestamp: " << e.what() << "\033[0m" << endl;
        }

        // Create filenames with camera serial and timestamp (if available)
        string rgb_filename, depth_filename, normal_rgb_filename;
        if (has_timestamp) {
          rgb_filename = (rgb_debug_dir / ("rgbd_" + _agent_id + "_t_" + timestamp + ".png")).string();
          depth_filename = (depth_debug_dir / ("depth_" + _agent_id + "_t_" + timestamp + ".png")).string();
          //normal_rgb_filename = (rgb_debug_dir / ("rgb_" + _agent_id + "_t_" + timestamp + ".png")).string();
        } else {
          rgb_filename = (rgb_debug_dir / ("rgbd_" + _agent_id + "_f_" + std::to_string(_global_frame_counter) + ".png")).string();
          depth_filename = (depth_debug_dir / ("depth_" + _agent_id + "_f_" + std::to_string(_global_frame_counter) + ".png")).string();
          //normal_rgb_filename = (rgb_debug_dir / ("rgb_" + _agent_id + "_f_" + std::to_string(_global_frame_counter) + ".png")).string();
        }
        
        // Save the RGB image (in depth coordinates)
        if (!_rgb.empty()) {
          cv::imwrite(rgb_filename, _rgb);
        }
        
        /*
        // Save the normal RGB image (original color coordinates)
        k4a::image k4a_color_image_original = _k4a_rgbd_capture.get_color_image();
        if (k4a_color_image_original.is_valid()) {
          cv::Mat rgb_original;
          if (k4a_color_image_to_cv_mat(k4a_color_image_original, rgb_original) == return_type::success) {
            cv::imwrite(normal_rgb_filename, rgb_original);
          }
        }
        */
        
        // Save the depth image
        if (!_rgbd.empty()) {
          cv::imwrite(depth_filename, _rgbd);
        }
      }
      #endif  
    }
    else if (_video_source == RGB_CAMERA_DUMMY || _video_source == RASPI_RGB_CAMERA_DUMMY){

    }
    else {
      cout << "Unknown video source type. Cannot acquire frame." << endl;
      return return_type::error;
    } 
    
    
    
    return return_type::success;
  }

  /**
   * @brief Compute the skeleton from the depth map.
   *
   * Compute the skeleton from the depth map. The resulting skeleton is stored
   * in #_skeleton3D attribute as a map of 3D points.
   *
   * @author Nicola
   * @return result status ad defined in return_type
   */
  return_type skeleton_from_depth_compute(bool debug = false) {

    if(_video_source == KINECT_AZURE_CAMERA || _video_source == KINECT_AZURE_DUMMY){
      #ifdef KINECT_AZURE_LIBS
      /*
      if (!_kinect_tracker.enqueue_capture(_k4a_rgbd_capture)) {
        // It should never hit timeout when K4A_WAIT_INFINITE is set.
        cout << "Error! Add capture to tracker process queue timeout!" << endl;
        return return_type::error;
      } 
      */

      _body_frame = _kinect_tracker.pop_result();
      
      if (_body_frame != nullptr) {
        if (_body_frame.get_num_bodies() == 0) {
            cout << "\033[1;34mNo bodies detected in the frame.\033[0m" << endl;
          return return_type::error;
        }
        
        // Only take one body (always the first one)
        k4abt_body_t body = _body_frame.get_body(0);

        _body_index_map = _body_frame.get_body_index_map();
        

        for (const auto &[index, keypoint_name] : keypoints_map_azure) {
          k4a_float3_t position = body.skeleton.joints[index].position;

          vector<float> keypoint_data;
          keypoint_data.push_back(static_cast<float>(position.v[0]));
          keypoint_data.push_back(static_cast<float>(position.v[1]));
          keypoint_data.push_back(static_cast<float>(position.v[2]));

          _skeleton3D[keypoint_name] = keypoint_data;
          
        }

        // Map the 3D keypoints to the 2D keypoints
        for (int i = 0; i < 18; ++i) {
          std::string keypoint_name_tmp = keypoints_map_openpose[i];
          for (const auto &[index, keypoint_azure_name] : keypoints_map_azure) {
            if (keypoint_azure_name == keypoint_name_tmp) {
              k4a_float3_t position = body.skeleton.joints[index].position;
              _keypoints_list_azure[i] =
                  cv::Point3f(position.v[0], position.v[1], position.v[2]);
              break;
            }
          }
        }

        if (debug) {
          cout << "\nSkeleton 3D:" << endl;
          for (const auto &[keypoint_name, keypoint_data] : _skeleton3D) {
            cout << keypoint_name << ": (";
            for (size_t i = 0; i < keypoint_data.size(); ++i) {
              cout << static_cast<int>(keypoint_data[i]);
              if (i < keypoint_data.size() - 1) {
                cout << ", ";
              }
            }
            cout << ")" << endl;
          }
        }

      } else {
        //  It should never hit timeout when K4A_WAIT_INFINITE is set.
        cout << "Error! Pop body frame result time out!" << endl;
        return return_type::error;
      }
      #endif  

    }
  
    return return_type::success;
  }

  /**
   * @brief Remove unnecessary points from the point cloud
   *
   * Make the point cloud lighter by removing unnecessary points, so that it
   * can be sent to the database via network
   *
   * @author Nicola
   * @return result status ad defined in return_type
   */
  return_type point_cloud_filter(bool debug = false, bool filter_enabled = true) {

    if(_video_source == KINECT_AZURE_CAMERA || _video_source == KINECT_AZURE_DUMMY){
      
      #ifdef KINECT_AZURE_LIBS
      
      if(_body_index_map != nullptr && filter_enabled){
        // mask the depth image with the body index map
        k4a::image masked_depth_image = k4a::image::create(
            K4A_IMAGE_FORMAT_DEPTH16, _k4a_depth_image.get_width_pixels(),
            _k4a_depth_image.get_height_pixels(), _k4a_depth_image.get_stride_bytes());

        mask_depth_with_body_index(_k4a_depth_image, _body_index_map,
                                  masked_depth_image);

        k4a_image_t masked_depth_handle = masked_depth_image.handle();
        k4a_image_reference(masked_depth_handle);

        // from k4a::image to cv::Mat --> depth image
        if (masked_depth_image != NULL) {
          // get raw buffer
          uint8_t *buffer = masked_depth_image.get_buffer();
          //cout << "Depth image buffer:" << buffer << endl;

          // convert the raw buffer to cv::Mat
          int rows = masked_depth_image.get_height_pixels();
          int cols = masked_depth_image.get_width_pixels();
          _rgbd_filtered = cv::Mat(rows, cols, CV_16U, (void *)buffer, cv::Mat::AUTO_STEP);
        }

        // convert the depth image to a point cloud
        // Take the color image without transforming it to depth coordinates
        k4a::image k4a_color_image_not_transformed = _k4a_rgbd_capture.get_color_image();
        k4a_image_t color_handle = k4a_color_image_not_transformed.handle();
        k4a_image_reference(color_handle);

        // create the point cloud and save it in _point_cloud variable
        create_point_cloud(_point_cloud_transformation, masked_depth_handle, color_handle);
        
        // Debug section: save filtered point cloud as PLY file
        if (debug) {
          // Create debug directory path relative to the source file location
          std::filesystem::path debug_dir = get_source_path();
          debug_dir = debug_dir / "Debug" / "point clouds";
          
          // Create debug directory if it doesn't exist
          try {
            if (!std::filesystem::exists(debug_dir)) {
              std::filesystem::create_directories(debug_dir);
            }
          } catch (const std::filesystem::filesystem_error& e) {
            cout << "\033[1;33mWarning: Failed to create point cloud debug directory: " << e.what() << "\033[0m" << endl;
          }
          
          // Create filename using global frame counter
          string ply_filename = (debug_dir / (std::to_string(_global_frame_counter) + "_filtered_point_cloud_frame" +  ".ply")).string();
          
          // Save the filtered point cloud to PLY file
          if (!_point_cloud.empty()) {
            write_ply_from_cv_mat(_point_cloud, ply_filename.c_str());
          }
        }
      } else{
        // Handle case when body index map is null OR when filtering is disabled
        if(_body_index_map == nullptr) {
          cout << "\033[1;34mBody index map is null. Creating unfiltered point cloud.\033[0m" << endl;
        } else {
          cout << "\033[1;34mPoint cloud filtering is disabled. Creating unfiltered point cloud.\033[0m" << endl;
        }
        
        // Create unfiltered point cloud using the original depth image
        k4a_image_t depth_handle = _k4a_depth_image.handle();
        k4a_image_reference(depth_handle);
        
        // Take the color image without transforming it to depth coordinates
        k4a::image k4a_color_image_not_transformed = _k4a_rgbd_capture.get_color_image();
        k4a_image_t color_handle = k4a_color_image_not_transformed.handle();
        k4a_image_reference(color_handle);

        // create the unfiltered point cloud and save it in _point_cloud variable
        create_point_cloud(_point_cloud_transformation, depth_handle, color_handle);
        
        // Debug section: save unfiltered point cloud as PLY file
        if (debug) {
          // Create debug directory path relative to the source file location
          std::filesystem::path debug_dir = get_source_path();
          debug_dir = debug_dir / "Debug" / "point clouds";
          
          // Create debug directory if it doesn't exist
          try {
            if (!std::filesystem::exists(debug_dir)) {
              std::filesystem::create_directories(debug_dir);
            }
          } catch (const std::filesystem::filesystem_error& e) {
            cout << "\033[1;33mWarning: Failed to create point cloud debug directory: " << e.what() << "\033[0m" << endl;
          }
          
          // Use the global frame counter
          string ply_filename = (debug_dir / (std::to_string(_global_frame_counter) + "_unfiltered_point_cloud_frame" +  ".ply")).string();
          
          // Save the unfiltered point cloud to PLY file
          if (!_point_cloud.empty()) {
            write_ply_from_cv_mat(_point_cloud, ply_filename.c_str());
          }
        }
      }
      #endif  
    }

    return return_type::success;
  }
  

  /**
   * @brief Transform the 3D skeleton coordinates in the global reference frame
   *
   * Use the extrinsic camera parameters to transorm the 3D skeleton coordinates
   * just before sending them as plugin output.
   *
   * @return return_type
   */
  return_type coordinate_transform(bool debug = false) {

    if (debug) {
      cout << "\033[1;34mApplying camera coordinate transformation...\033[0m" << endl;
    }

    // Transform each keypoint in the 3D skeleton
    for (auto& [joint_name, joint_3d] : _skeleton3D) {
      if (joint_3d.size() >= 3) {
        // Apply the camera transformation matrix
        vector<float> joint_3d_transformed = apply_camera_transformation(joint_3d);
        _skeleton3D[joint_name] = joint_3d_transformed;
      }
    }

    // Transform each keypoint in the 3D skeleton
    for (auto& [joint_name, cov_3d] : _cov3D) {
      if (cov_3d.size() >= 3) {
        // Apply the camera transformation matrix
        Eigen::Matrix3f cov_3d_transformed = apply_camera_transformation(cov_3d);
        _cov3D[joint_name] = cov_3d_transformed;
      }
    }

    if (debug) {
      cout << "\nTransformed Skeleton 3D:" << endl;
      for (const auto &[keypoint_name, keypoint_data] : _skeleton3D) {
        cout << keypoint_name << ": (";
        for (size_t i = 0; i < keypoint_data.size(); ++i) {
          cout << static_cast<int>(keypoint_data[i]);
          if (i < keypoint_data.size() - 1) {
            cout << ", ";
          }
        }
        cout << ")" << endl;
      }
    }

    return return_type::success;
  }
  

  /**
   * @brief Compute the skeleton from RGB images only
   *
   * Compute the skeleton from RGB images only. On success, the field
   * #_skeleton2D is updated (as a map of 2D points).
   * Also, the field #_heatmaps is updated with the joints heatmaps (one per
   * joint).
   *
   * There is a configuration flag for optionally skipping this branch
   * on Azure agents.
   *
   * @author Alessandro
   * @return result status ad defined in return_type
   */
  return_type skeleton_from_rgb_compute(bool debug = false) {

    if (_video_source != UNKNOWN){
      
      // Check if pipeline is valid
      if (!_pipeline) {
        cout << "\033[1;31mError: Pipeline is not initialized\033[0m" << endl;
        return return_type::error;
      }
      
      if (_pipeline->isReadyToProcess()) {
        if (_rgb.empty()) {
          cout << "\033[1;31mError: RGB image is empty\033[0m" << endl;
          return return_type::error;
        }

        _frame_num = _pipeline->submitData(ImageInputData(_rgb), make_shared<ImageMetaData>(_rgb, _frame_time));
      } else {
        cout << "\033[1;33mWarning: Pipeline is not ready to process\033[0m" << endl;
        return return_type::warning;
      }

      // Implement a simple timeout mechanism
      auto start_time = std::chrono::steady_clock::now();
      const auto timeout_duration = std::chrono::seconds(10); // 10 second timeout
      
      try {
        // Try to wait for data with a manual timeout check
        while (true) {
          auto current_time = std::chrono::steady_clock::now();
          if (current_time - start_time > timeout_duration) {
            cout << "\033[1;31mTimeout waiting for pipeline data (10 seconds)\033[0m" << endl;
            return return_type::error;
          }
          
          // Check if data is available
          _result = _pipeline->getResult();
          if (_result) {
            break;
          }
          
          // Small sleep to avoid busy waiting
          std::this_thread::sleep_for(std::chrono::milliseconds(50));
        }
        
      } catch (const std::exception& e) {
        cout << "\033[1;31mException in getting pipeline result: " << e.what() << "\033[0m" << endl;
        return return_type::error;
      }

      if (debug) {
        try {
          _rgb = renderHumanPose(_result->asRef<HumanPoseResult>(), _output_transform);
        } catch (const std::exception& e) {
          cout << "\033[1;33mWarning: Failed to render pose: " << e.what() << "\033[0m" << endl;
        }
      }
      _frames_processed++;
    
    return return_type::success;
    }
    else{
      cout << "\033[1;31mUnknown video source type. Cannot compute skeleton from RGB.\033[0m" << endl;
      return return_type::error;
    }
  }

  /**
   * @brief Compute the hessians for joints
   *
   * Compute the hessians for joints on the RGB frame based on the #_heatmaps
   * field.
   *
   * @author Alessandro
   * @return result status ad defined in return_type
   */
  return_type hessian_compute(bool debug = false) {
    
    if (_video_source != UNKNOWN){
      size_t n_pixel = 10; // of how many pixels I move

      _keypoints_list_openpose.clear();
      _keypoints_cov_openpose.resize(HPEOpenPose::keypointsNumber);
      _keypoints_cov_openpose.clear();
      _poses_openpose.clear();
      _poses_openpose = _result->asRef<HumanPoseResult>().poses;


      // cout << "poses.size()-----> " << _poses_openpose.size() << endl;
      if (_poses_openpose.size() > 0) { // at least one person
        
        for (auto &keypoint :
            _poses_openpose[0].keypoints) { // if I have more than one person, I take the
          // first with id[0]

          if (keypoint.x > _rgb_width) {
            keypoint.x = _rgb_width - 1;
          }
          if (keypoint.y > _rgb_height) {
            keypoint.y = _rgb_height - 1;
          }
          _keypoints_list_openpose.push_back(cv::Point2i(
              keypoint.x,
              keypoint.y)); // I always have 18 keypoints, if there is no (-1,-1)
        }

        for (int ii = 0; ii < HPEOpenPose::keypointsNumber; ii++) {

          if (_keypoints_list_openpose[ii].x > 0 && _keypoints_list_openpose[ii].y > 0) {

            if (_keypoints_list_openpose[ii].y < n_pixel) {
              _keypoints_list_openpose[ii].y = n_pixel;
            } else if (_keypoints_list_openpose[ii].y >= _rgb_height - n_pixel) {
              _keypoints_list_openpose[ii].y = _rgb_height - n_pixel - 1;
            }

            if (_keypoints_list_openpose[ii].x < n_pixel) {
              _keypoints_list_openpose[ii].x = n_pixel;
            } else if (_keypoints_list_openpose[ii].x >= _rgb_width - n_pixel) {
              _keypoints_list_openpose[ii].x = _rgb_width - n_pixel - 1;
            }

            cv::Mat _heat_map = _result->asRef<HumanPoseResult>().heatMaps[ii];
            cv::resize(_heat_map, _heat_map, cv::Size(_rgb_width, _rgb_height));

            data_t H_ri_ci = _heat_map.at<data_t>(_keypoints_list_openpose[ii].y,
                                                  _keypoints_list_openpose[ii].x);
            
            data_t H_ri_ciPLUSn = _heat_map.at<data_t>(
                _keypoints_list_openpose[ii].y, _keypoints_list_openpose[ii].x + n_pixel);
              
            data_t H_ri_ciMINn = _heat_map.at<data_t>(
                _keypoints_list_openpose[ii].y, _keypoints_list_openpose[ii].x - n_pixel);
            
            data_t H_riPLUSn_ci = _heat_map.at<data_t>(
                _keypoints_list_openpose[ii].y + n_pixel, _keypoints_list_openpose[ii].x);
            
            data_t H_riMINn_ci = _heat_map.at<data_t>(
                _keypoints_list_openpose[ii].y - n_pixel, _keypoints_list_openpose[ii].x);
            

            data_t H11 = (1.0 / (n_pixel * n_pixel)) *
                        (H_ri_ciPLUSn - 2 * H_ri_ci + H_ri_ciMINn);
            
            data_t H22 = (1.0 / (n_pixel * n_pixel)) *
                        (H_riPLUSn_ci - 2 * H_ri_ci + H_riMINn_ci);
            

            data_t H_riMINn_ciMINn = _heat_map.at<data_t>(
                _keypoints_list_openpose[ii].y - n_pixel, _keypoints_list_openpose[ii].x - n_pixel);
            
            data_t H_riMINn_ciPLUSn = _heat_map.at<data_t>(
                _keypoints_list_openpose[ii].y - n_pixel, _keypoints_list_openpose[ii].x + n_pixel);
            
            data_t H_riPLUSn_ciMINn = _heat_map.at<data_t>(
                _keypoints_list_openpose[ii].y + n_pixel, _keypoints_list_openpose[ii].x - n_pixel);
            
            data_t H_riPLUSn_ciPLUSn = _heat_map.at<data_t>(
                _keypoints_list_openpose[ii].y + n_pixel, _keypoints_list_openpose[ii].x + n_pixel);
            

            data_t H12 = (1.0 / (4 * n_pixel * n_pixel)) *
                        (H_riPLUSn_ciPLUSn - H_riPLUSn_ciMINn -
                          H_riMINn_ciPLUSn + H_riMINn_ciMINn);
            
            data_t H21 = H12;

            Eigen::Matrix2f A;
            A << H11, H12, H21, H22;

            

            Eigen::Matrix2f C = (-A).inverse();
          
            _cov2D_vec.push_back(C);
            Eigen::EigenSolver<Eigen::Matrix2f> s(C); // the instance s(C)
                                                      // includes the eigensystem

            complex<data_t> D11_tmp = s.eigenvalues()[0];
            data_t D11 = D11_tmp.real(); 
            
            complex<data_t> D22_tmp = s.eigenvalues()[1];
            data_t D22 = D22_tmp.real();
            

            complex<data_t> V11_tmp = s.eigenvectors()(0, 0);
            data_t V11 = V11_tmp.real();
            complex<data_t> V21_tmp = s.eigenvectors()(1, 0);
            data_t V21 = V21_tmp.real();

            data_t perc_prob = 0.68; // standard confidence interval
            data_t xradius = sqrt(D11 * (-2) * log(1 - perc_prob));
            data_t yradius = sqrt(D22 * (-2) * log(1 - perc_prob));

            data_t alpha = atan2(V21, V11);

            _keypoints_cov_openpose[ii].x = xradius;
            _keypoints_cov_openpose[ii].y = yradius;
            _keypoints_cov_openpose[ii].z = alpha;

            if (debug) {
              vector<data_t> theta;
              for (data_t j = 0; j < 2 * M_PI; j += 2 * (M_PI / 40)) {
                theta.push_back(j);
              }

              vector<data_t> x_ellips;
              vector<data_t> y_ellips;

              for (int j = 0; j < theta.size(); j++) {
                x_ellips.push_back(xradius * cos(theta[j]));
                y_ellips.push_back(yradius * sin(theta[j]));
              }

              vector<cv::Point2f> ellipse_points;
              for (int j = 0; j < x_ellips.size(); j++) {
                data_t element_1 =
                    (cos(alpha) * x_ellips[j] + (-sin(alpha)) * y_ellips[j]) +
                    _keypoints_list_openpose[ii].x;
                data_t element_2 =
                    (sin(alpha) * x_ellips[j] + cos(alpha) * y_ellips[j]) +
                    _keypoints_list_openpose[ii].y;
                ellipse_points.push_back(cv::Point2f(element_1, element_2));
              }

              // Draw keypoints
              cv::circle(_rgb,
                        cv::Point(_keypoints_list_openpose[ii].x, _keypoints_list_openpose[ii].y),
                        5, cv::Scalar(0, 255, 0), cv::FILLED);

              // Draw ellipse points
              for (int i = 0; i < ellipse_points.size(); ++i) {

                if (ellipse_points[i].x < 0) {
                  ellipse_points[i].x = 1;
                }
                if (ellipse_points[i].y < 0) {
                  ellipse_points[i].y = 1;
                }
                if (ellipse_points[i].x > _rgb_width) {
                  ellipse_points[i].x = _rgb_width - 1;
                }
                if (ellipse_points[i].y > _rgb_height) {
                  ellipse_points[i].y = _rgb_height - 1;
                }

                cv::circle(_rgb, ellipse_points[i], 2, cv::Scalar(255, 0, 0),
                          cv::FILLED, 8, 0);
              }
            }
          } else {
            Eigen::Matrix2f A_NaN;
            A_NaN << -1, -1, -1, -1;
            _cov2D_vec.push_back(A_NaN);
          }
        }
      }
      return return_type::success;
    } 
    else {
      cout << "\033[1;31mUnknown video source type. Cannot compute hessians.\033[0m" << endl;
      return return_type::error;
    }
  
  }

  /**
   * @brief Compute the 3D covariance matrix
   *
   * Compute the 3D covariance matrix.
   * Two possible cases:
   *   1. one Azure camera: use the 3D to uncertainty in the view axis, use
   *      the 2D image to uncertainty in the projection plane
   *   2. one RGB camera: calculates a 3D ellipsoid based on the 2D covariance
   *      plus the "reasonable" depth range as a third azis (direction of view)
   *
   * @author Alessandro
   * @return result status ad defined in return_type
   */
  return_type cov3D_compute(bool debug = false) {

    if (_video_source == KINECT_AZURE_CAMERA || _video_source == KINECT_AZURE_DUMMY) {
      #ifdef KINECT_AZURE_LIBS
      if (_keypoints_list_openpose.size() > 0) { // at least one person
        for (size_t i = 0; i < _cov2D_vec.size(); ++i) {
          Eigen::Matrix2f _cov2D_vec_TMP = _cov2D_vec[i];

          if (!(_keypoints_list_azure[i].x == -1 &&
                _keypoints_list_azure[i].y == -1 &&
                _keypoints_list_azure[i].z == -1) &&
              !(_cov2D_vec_TMP.array() == -1).all()) {

            float Z_tmp = _keypoints_list_azure[i].z;
            // Matteo faccina arrabbiata perchè hardcoded
            float sigma_z = -0.000000000035541*Z_tmp*Z_tmp*Z_tmp+0.000000493877878*Z_tmp*Z_tmp - 0.001100245600022*Z_tmp+1.989206937961068; // KINECT AZURE model
            float variance_z = sigma_z * sigma_z;

            Eigen::Matrix<float, 3, 2> J;
            J << Z_tmp / _fx, 0, 0, Z_tmp / _fy, 0, 0;

            Eigen::Matrix3f covMatrixZ;
            covMatrixZ << 0, 0, 0, 0, 0, 0, 0, 0, variance_z;

            Eigen::Matrix3f covMatrix3D = J * _cov2D_vec_TMP * J.transpose() + covMatrixZ;
            _cov3D_vec.push_back(covMatrix3D);
            _cov3D[keypoints_map_openpose[i]] = covMatrix3D; // store the 3D covariance matrix in the map
          } else {
            Eigen::Matrix3f covMatrixZ_NaN;
            covMatrixZ_NaN.setConstant(-1);
            _cov3D_vec.push_back(covMatrixZ_NaN);
            _cov3D[keypoints_map_openpose[i]] = covMatrixZ_NaN; // store the 3D covariance matrix in the map
          }
        }
      }

      if (debug) {
        // Save the 3D covariance matrices to a JSON file
        json json_cov3D_vec;
        
        // Create debug directory path relative to the source file location
        std::filesystem::path debug_dir = get_source_path();
        debug_dir = debug_dir / "Debug" / "Covariances 3D";
        
        // Create debug directory if it doesn't exist
        try {
          if (!std::filesystem::exists(debug_dir)) {
            std::filesystem::create_directories(debug_dir);
            cout << "Created covariances debug directory: " << debug_dir << endl;
          }
        } catch (const std::filesystem::filesystem_error& e) {
          cout << "\033[1;33mWarning: Failed to create covariances debug directory: " << e.what() << "\033[0m" << endl;
        }
        
        string cov3D_filename = (debug_dir / "cov3D_data.json").string();
        
        std::ifstream input_file(cov3D_filename);

        if (input_file.is_open()) {
          input_file >> json_cov3D_vec;
          input_file.close();
        }

        json frame_data;
        frame_data["frame"] = _global_frame_counter;
        json cov_matrix_list;

        for (size_t i = 0; i < _cov3D_vec.size(); ++i) {
          json cov_matrix_3x3;

          for (int row = 0; row < 3; ++row) {
            cov_matrix_3x3.push_back(
                {_cov3D_vec[i](row, 0), _cov3D_vec[i](row, 1), _cov3D_vec[i](row, 2)});
          }

          cov_matrix_list.push_back(cov_matrix_3x3);
        }

        frame_data["cov3D"] = cov_matrix_list;

        json_cov3D_vec.push_back(frame_data);

        std::ofstream output_file(cov3D_filename);
        output_file << json_cov3D_vec.dump(4);
        output_file.close();
      }
      #endif

      return return_type::success;
    }
    
    else if (_video_source == RASPI_RGB_CAMERA || _video_source == RASPI_RGB_CAMERA_DUMMY) {

      // RASPI Intrinsic parameters
      float f_mm =
          6; // focal length in mm
             // https://grobotronics.com/raspberry-pi-hq-camera-lens-6mm-wide-angle.html?sl=en
      // Sensor dimensions:
      float d_x = (4056 * 1.55) /
                  1000; // https://www.waveshare.com/wiki/Raspberry_Pi_HQ_Camera
      float d_y = (3040 * 1.55) / 1000;

      int H = _rgb.rows; // image size after resize
      int W = _rgb.cols;

      _fx = (f_mm * W) / d_x; // focal length in pixel
      _fy = (f_mm * H) / d_y; // focal length in pixel

      _cx = W / 2; // coordinate of the principal point
      _cy = H / 2;

      float Hp =
          500; // Real "height" of the selected person in mm between nec and hip
      float sigmaHp = 2; // mm

      if (_keypoints_list_openpose.size() > 0) { // at least one person
        if (((_keypoints_list_openpose[2].y > 0) || (_keypoints_list_openpose[5].y > 0)) &&
            ((_keypoints_list_openpose[8].y > 0) ||
             (_keypoints_list_openpose[11].y >
              0))) { // 2 = SHOR      5 = SHOL     8 = HIPR       11 = HIPL

          float v_sho_tmp;
          float v_sho;
          Eigen::Matrix2f cov2D_SHO;
          if (_keypoints_list_openpose[2].y > 0 && _keypoints_list_openpose[5].y > 0) {
            v_sho_tmp =
                fabs(_keypoints_list_openpose[2].y - _keypoints_list_openpose[5].y) / 2.0f;
            if (_keypoints_list_openpose[2].y < _keypoints_list_openpose[5].y) {
              v_sho = v_sho_tmp + _keypoints_list_openpose[2].y;
            } else {
              v_sho = v_sho_tmp + _keypoints_list_openpose[5].y;
            }
            cov2D_SHO = (_cov2D_vec[2] + _cov2D_vec[5]) / 2.0;
          } else if (_keypoints_list_openpose[2].y > 0) {
            v_sho = _keypoints_list_openpose[2].y;
            cov2D_SHO = _cov2D_vec[2];
          } else {
            v_sho = _keypoints_list_openpose[5].y;
            cov2D_SHO = _cov2D_vec[5];
          }
          float variance_shoY = cov2D_SHO(1, 1);

          float v_hip_tmp;
          float v_hip;
          Eigen::Matrix2f cov2D_HIP;
          if (_keypoints_list_openpose[8].y > 0 && _keypoints_list_openpose[11].y > 0) {
            v_hip_tmp =
                fabs(_keypoints_list_openpose[8].y - _keypoints_list_openpose[11].y) / 2.0f;
            if (_keypoints_list_openpose[8].y < _keypoints_list_openpose[11].y) {
              v_hip = v_hip_tmp + _keypoints_list_openpose[8].y;
            } else {
              v_hip = v_hip_tmp + _keypoints_list_openpose[11].y;
            }
            cov2D_HIP = (_cov2D_vec[11] + _cov2D_vec[8]) / 2.0;
          } else if (_keypoints_list_openpose[8].y > 0) {
            v_hip = _keypoints_list_openpose[8].y;
            cov2D_HIP = _cov2D_vec[8];
          } else {
            v_hip = _keypoints_list_openpose[11].y;
            cov2D_HIP = _cov2D_vec[11];
          }
          float variance_hipY = cov2D_HIP(1, 1);

          float Z = (_fy * Hp) / fabs(v_hip - v_sho);

          // cout << "Z---------->    " << Z << endl;

          for (size_t i = 0; i < _cov2D_vec.size(); ++i) {
            Eigen::Matrix2f cov2D_TMP = _cov2D_vec[i];
            if (!(cov2D_TMP.array() == -1).all()) {
              Eigen::Matrix<float, 3, 2> J;
              J << Z / _fx, 0, 0, Z / _fy, 0, 0;

              float variance_z_Hp =
                  pow(_fy / fabs(v_hip - v_sho), 2) * pow(sigmaHp, 2);
              float variance_z_sho =
                  pow(_fy * Hp / pow(v_hip - v_sho, 2), 2) * variance_shoY;
              float variance_z_hip =
                  pow(_fy * Hp / pow(v_hip - v_sho, 2), 2) * variance_hipY;
              float variance_z =
                  variance_z_Hp + variance_z_sho + variance_z_hip;

              Eigen::Matrix3f covMatrixZ;
              covMatrixZ << 0, 0, 0, 0, 0, 0, 0, 0, variance_z;

              Eigen::Matrix3f covMatrix3D =
                  J * cov2D_TMP * J.transpose() + covMatrixZ;
              _cov3D_vec.push_back(covMatrix3D);
              _cov3D[keypoints_map_openpose[i]] = covMatrix3D; // store the 3D covariance matrix in the map
            } else {
              Eigen::Matrix3f covMatrixZ_NaN;
              covMatrixZ_NaN.setConstant(-1);
              _cov3D_vec.push_back(covMatrixZ_NaN);
              _cov3D[keypoints_map_openpose[i]] = covMatrixZ_NaN; // store the 3D covariance matrix in the map
            }
          }
        }
      }
      
      return return_type::success;
    }

    else if (_video_source == RGB_CAMERA || _video_source == RGB_CAMERA_DUMMY){

      // RGB Camera Intrinsic parameters
      // TODO: Calibrate RGB Camera and take intrinsic param from INI
      float f_mm =
          6; // focal length in mm
             // https://grobotronics.com/raspberry-pi-hq-camera-lens-6mm-wide-angle.html?sl=en
      // Sensor dimensions:
      float d_x = (4056 * 1.55) /
                  1000; // https://www.waveshare.com/wiki/Raspberry_Pi_HQ_Camera
      float d_y = (3040 * 1.55) / 1000;

      int H = _rgb.rows; // image size after resize
      int W = _rgb.cols;

      _fx = (f_mm * W) / d_x; // focal length in pixel
      _fy = (f_mm * H) / d_y; // focal length in pixel

      _cx = W / 2; // coordinate of the principal point
      _cy = H / 2;

      float Hp =
          500; // Real "height" of the selected person in mm between nec and hip
      float sigmaHp = 2; // mm

      if (_keypoints_list_openpose.size() > 0) { // at least one person
        if (((_keypoints_list_openpose[2].y > 0) || (_keypoints_list_openpose[5].y > 0)) &&
            ((_keypoints_list_openpose[8].y > 0) ||
             (_keypoints_list_openpose[11].y >
              0))) { // 2 = SHOR      5 = SHOL     8 = HIPR       11 = HIPL

          float v_sho_tmp;
          float v_sho;
          Eigen::Matrix2f cov2D_SHO;
          if (_keypoints_list_openpose[2].y > 0 && _keypoints_list_openpose[5].y > 0) {
            v_sho_tmp =
                fabs(_keypoints_list_openpose[2].y - _keypoints_list_openpose[5].y) / 2.0f;
            if (_keypoints_list_openpose[2].y < _keypoints_list_openpose[5].y) {
              v_sho = v_sho_tmp + _keypoints_list_openpose[2].y;
            } else {
              v_sho = v_sho_tmp + _keypoints_list_openpose[5].y;
            }
            cov2D_SHO = (_cov2D_vec[2] + _cov2D_vec[5]) / 2.0;
          } else if (_keypoints_list_openpose[2].y > 0) {
            v_sho = _keypoints_list_openpose[2].y;
            cov2D_SHO = _cov2D_vec[2];
          } else {
            v_sho = _keypoints_list_openpose[5].y;
            cov2D_SHO = _cov2D_vec[5];
          }
          float variance_shoY = cov2D_SHO(1, 1);

          float v_hip_tmp;
          float v_hip;
          Eigen::Matrix2f cov2D_HIP;
          if (_keypoints_list_openpose[8].y > 0 && _keypoints_list_openpose[11].y > 0) {
            v_hip_tmp =
                fabs(_keypoints_list_openpose[8].y - _keypoints_list_openpose[11].y) / 2.0f;
            if (_keypoints_list_openpose[8].y < _keypoints_list_openpose[11].y) {
              v_hip = v_hip_tmp + _keypoints_list_openpose[8].y;
            } else {
              v_hip = v_hip_tmp + _keypoints_list_openpose[11].y;
            }
            cov2D_HIP = (_cov2D_vec[11] + _cov2D_vec[8]) / 2.0;
          } else if (_keypoints_list_openpose[8].y > 0) {
            v_hip = _keypoints_list_openpose[8].y;
            cov2D_HIP = _cov2D_vec[8];
          } else {
            v_hip = _keypoints_list_openpose[11].y;
            cov2D_HIP = _cov2D_vec[11];
          }
          float variance_hipY = cov2D_HIP(1, 1);

          float Z = (_fy * Hp) / fabs(v_hip - v_sho);

          // cout << "Z---------->    " << Z << endl;

          for (size_t i = 0; i < _cov2D_vec.size(); ++i) {
            Eigen::Matrix2f cov2D_TMP = _cov2D_vec[i];
            if (!(cov2D_TMP.array() == -1).all()) {
              Eigen::Matrix<float, 3, 2> J;
              J << Z / _fx, 0, 0, Z / _fy, 0, 0;

              float variance_z_Hp =
                  pow(_fy / fabs(v_hip - v_sho), 2) * pow(sigmaHp, 2);
              float variance_z_sho =
                  pow(_fy * Hp / pow(v_hip - v_sho, 2), 2) * variance_shoY;
              float variance_z_hip =
                  pow(_fy * Hp / pow(v_hip - v_sho, 2), 2) * variance_hipY;
              float variance_z =
                  variance_z_Hp + variance_z_sho + variance_z_hip;

              Eigen::Matrix3f covMatrixZ;
              covMatrixZ << 0, 0, 0, 0, 0, 0, 0, 0, variance_z;

              Eigen::Matrix3f covMatrix3D =
                  J * cov2D_TMP * J.transpose() + covMatrixZ;
              _cov3D_vec.push_back(covMatrix3D);
              _cov3D[keypoints_map_openpose[i]] = covMatrix3D; // store the 3D covariance matrix in the map
            } else {
              Eigen::Matrix3f covMatrixZ_NaN;
              covMatrixZ_NaN.setConstant(-1);
              _cov3D_vec.push_back(covMatrixZ_NaN);
              _cov3D[keypoints_map_openpose[i]] = covMatrixZ_NaN; // store the 3D covariance matrix in the map
            }
          }
        }
      }
      return return_type::success;
    } 
    else {
      cout << "\033[1;31mUnknown video source type. Cannot compute 3D covariance.\033[0m" << endl;
      return return_type::error;
    }
  }

  /**
   * @brief Consistency check of the 3D skeleton according to human physiology
   *
   * @authors Marco, Matteo
   * @return result status ad defined in return_type
   */
  return_type consistency_check(bool debug = false) {

    cout << "Performing consistency check..." << endl;
    
    return return_type::success;
  } 

  return_type viewer(bool debug = false) {

    if (debug) {
      if (_video_source == KINECT_AZURE_CAMERA || _video_source == KINECT_AZURE_DUMMY) {

        // Show _rgb and _rgbd images in two separate windows contempouraneously
        //cv::namedWindow("RGB Frame", cv::WINDOW_NORMAL);
        //cv::namedWindow("RGBD Frame", cv::WINDOW_NORMAL);

        cv::imshow("RGB Frame", _rgb);

        int MAX_DEPTH = 6000; // Maximum depth value in mm
        if(!_rgbd_filtered.empty()){
          _rgbd_filtered.convertTo(_rgbd_filtered, CV_8U, 255.0 / MAX_DEPTH); 
          Mat rgbd_color_filtered;
          applyColorMap(_rgbd_filtered, rgbd_color_filtered, COLORMAP_HSV);

          cv::imshow("RGBD Frame", rgbd_color_filtered);
        }
        else{
          _rgbd.convertTo(_rgbd, CV_8U, 255.0 / MAX_DEPTH); 
          Mat rgbd_color;
          applyColorMap(_rgbd, rgbd_color, COLORMAP_HSV);

          cv::imshow("RGBD Frame", rgbd_color);
        }
        
        // Wait for a key press for 30 milliseconds
        int key = cv::waitKey(30);
        if (key == 27) { // If 'ESC' key is pressed
          cout << "Exiting viewer..." << endl;
          cv::destroyAllWindows();
          return return_type::error;
        }

      }
      else if (_video_source == RGB_CAMERA || _video_source == RASPI_RGB_CAMERA || 
              _video_source == RGB_CAMERA_DUMMY || _video_source == RASPI_RGB_CAMERA_DUMMY) {

        // Show _rgb image in a window
        //cv::namedWindow("RGB Frame", cv::WINDOW_NORMAL);
        cv::imshow("RGB Frame", _rgb);

        // Wait for a key press for 30 milliseconds
        int key = cv::waitKey(30);

        if (key == 27) { // If 'ESC' key is pressed
          cout << "Exiting viewer..." << endl;
          cv::destroyAllWindows();
          return return_type::error;
        }

      }
      else {
        cout << "Unknown video source type. Cannot launch viewer." << endl;
        return return_type::error;
      }
    }
    
    return return_type::success;
  }

  // Typically, no need to change this
  string kind() override { return PLUGIN_NAME; }

  // Implement the actual functionality here
  return_type get_output(json &out, std::vector<unsigned char> *blob = nullptr) override {

    out.clear();

    auto start_get_output = std::chrono::high_resolution_clock::now();

    auto time_prev = std::chrono::high_resolution_clock::now();

    if (!_agent_id.empty()) out["agent_id"] = _agent_id;

    if (acquire_frame(_params["debug"]["acquire_frame"]) == return_type::error) {
      return return_type::error;
    }

    // Update frame timestamp after acquiring frame
    out["ts"] = std::chrono::duration_cast<std::chrono::nanoseconds>(_frame_time.time_since_epoch()).count();
    
    auto time_now = std::chrono::high_resolution_clock::now();
    auto duration = std::chrono::duration_cast<std::chrono::microseconds>(time_now - time_prev);
    std::cout << "Acquire frame: " << duration.count() << " microseconds" << endl;
    time_prev = time_now;

    #ifdef KINECT_AZURE_LIBS
    if (!_kinect_tracker.enqueue_capture(_k4a_rgbd_capture)) {
      // It should never hit timeout when K4A_WAIT_INFINITE is set.
      cout << "Error! Add capture to tracker process queue timeout!" << endl;
      return return_type::error;
    } 
    #endif

    time_now = std::chrono::high_resolution_clock::now();
    duration = std::chrono::duration_cast<std::chrono::microseconds>(time_now - time_prev);
    std::cout << "Skeleton call: " << duration.count() << " microseconds" << endl;
    time_prev = time_now;

    if (skeleton_from_rgb_compute(_params["debug"]["skeleton_from_rgb_compute"]) == return_type::error) {
      return return_type::error;
    }

    time_now = std::chrono::high_resolution_clock::now();
    duration = std::chrono::duration_cast<std::chrono::microseconds>(time_now - time_prev);
    std::cout << "Skeleton rgb: " << duration.count() << " microseconds" << endl;
    time_prev = time_now;

    if (hessian_compute(_params["debug"]["hessian_compute"]) == return_type::error) {
      return return_type::error;
    }

    time_now = std::chrono::high_resolution_clock::now();
    duration = std::chrono::duration_cast<std::chrono::microseconds>(time_now - time_prev);
    std::cout << "Hessian compute: " << duration.count() << " microseconds" << endl;
    time_prev = time_now;

    if (cov3D_compute(_params["debug"]["cov3D_compute"]) == return_type::error) {
      return return_type::error;
    }

    time_now = std::chrono::high_resolution_clock::now();
    duration = std::chrono::duration_cast<std::chrono::microseconds>(time_now - time_prev);
    std::cout << "Cov compute: " << duration.count() << " microseconds" << endl;
    time_prev = time_now;

    if (skeleton_from_depth_compute(_params["debug"]["skeleton_from_depth_compute"]) == return_type::error) {
      return return_type::error;
    }
    
    time_now = std::chrono::high_resolution_clock::now();
    duration = std::chrono::duration_cast<std::chrono::microseconds>(time_now - time_prev);
    std::cout << "Skeleton depth: " << duration.count() << " microseconds" << endl;
    time_prev = time_now;

    if (point_cloud_filter(_params["debug"]["point_cloud_filter"],  _params.value("filter_point_cloud", true)) == return_type::error) {
      return return_type::error;
    }

    time_now = std::chrono::high_resolution_clock::now();
    duration = std::chrono::duration_cast<std::chrono::microseconds>(time_now - time_prev);
    std::cout << "Point cloud filter: " << duration.count() << " microseconds" << endl;
    time_prev = time_now;

    if (coordinate_transform(_params["debug"]["coordinate_transform"]) == return_type::error) {
      return return_type::error;
    }

    time_now = std::chrono::high_resolution_clock::now();
    duration = std::chrono::duration_cast<std::chrono::microseconds>(time_now - time_prev);
    std::cout << "Coordinate transform: " << duration.count() << " microseconds" << endl;
    time_prev = time_now;

    if (viewer(_params["debug"]["viewer"]) == return_type::error) {
      return return_type::error;
    }
    
    time_now = std::chrono::high_resolution_clock::now();
    duration = std::chrono::duration_cast<std::chrono::microseconds>(time_now - time_prev);
    std::cout << "Viewer: " << duration.count() << " microseconds" << endl;
    time_prev = time_now;


    // Prepare output 
    // the json definition is on the google doc: https://docs.google.com/document/d/1IRs9VA9gGh8CmGIK8cRInYrMCY8cF8FMGC5H8DoonJI

    // Use maps to convert from keypoints lists to  _skeleton3D and _cov3D
    // _skeleton3D is already in the right format, so we can use it directly
    // _cov3D_vec is a vector of matrices, so we need to convert it to a MAP in _cov3D

    // Prepare the output json
    if (_poses_openpose.size() > 0) {
      out["typ"] = "3D";
      for (int kp = 0; kp < _keypoints_list_openpose.size(); kp++) {
        if (_keypoints_list_openpose[kp].x < 0 || _keypoints_list_openpose[kp].y < 0)
          continue;
        
        if (_video_source == KINECT_AZURE_CAMERA || _video_source == KINECT_AZURE_DUMMY) {
          
          out[keypoints_map_openpose[kp]]["ncm"] = 1; // number of cameras used, constantantly 1 for one HPE plugin

          out[keypoints_map_openpose[kp]]["crd"] = _skeleton3D[keypoints_map_openpose[kp]];
         
          out[keypoints_map_openpose[kp]]["unc"] = 
            {
              _cov3D[keypoints_map_openpose[kp]](0, 0),   // Ux
              _cov3D[keypoints_map_openpose[kp]](1, 1),   // Uy
              _cov3D[keypoints_map_openpose[kp]](2, 2),   // Uz
              _cov3D[keypoints_map_openpose[kp]](0, 1),   // Uxy
              _cov3D[keypoints_map_openpose[kp]](0, 2),   // Uxz
              _cov3D[keypoints_map_openpose[kp]](1, 2)    // Uyz
            }; 

        } 
        else if (_video_source == RGB_CAMERA || _video_source == RASPI_RGB_CAMERA ||
                   _video_source == RGB_CAMERA_DUMMY || _video_source == RASPI_RGB_CAMERA_DUMMY) {
            //used ony RGB inference

            //TODO: CONVERTIRE I PUNTI IN 3D!!!! Usare la matrice di calibrazione (ALE LUCHETTI)
            
            out["typ"] = "2D";

            out[keypoints_map_openpose[kp]]["crd"] = {
              _keypoints_list_openpose[kp].x,
              _keypoints_list_openpose[kp].y};

            out[keypoints_map_openpose[kp]]["unc"] = {
              _keypoints_cov_openpose[kp].x,
              _keypoints_cov_openpose[kp].y,
              _keypoints_cov_openpose[kp].z
            };
        }
        else {
          cout << "\033[1;31mUnknown video source type. Cannot prepare output.\033[0m" << endl;
          return return_type::error;
        }
      }
    }

    time_now = std::chrono::high_resolution_clock::now();
    duration = std::chrono::duration_cast<std::chrono::microseconds>(time_now - time_prev);
    std::cout << "Output prepare: " << duration.count() << " microseconds" << endl;
    time_prev = time_now;


    // clear the fields for the next frame
    _skeleton2D.clear();
    _skeleton3D.clear(); 
    _cov3D.clear();
    _cov2D_vec.clear();
    _cov3D_vec.clear();
    
    time_now = std::chrono::high_resolution_clock::now();
    duration = std::chrono::duration_cast<std::chrono::microseconds>(time_now - time_prev);
    std::cout << "Output clear: " << duration.count() << " microseconds" << endl;

    auto stop_get_output = std::chrono::high_resolution_clock::now();
    duration = std::chrono::duration_cast<std::chrono::microseconds>(stop_get_output - start_get_output);
    std::cout << "GET OUTPUT TIME: " << duration.count() << " microseconds" << endl;

    return return_type::success;
  }

  void set_params(void const *params) override {
    Source::set_params(params);
    _params.merge_patch(*(json *)params);

    if (_params.contains("resolution_rgb")) {
      _resolution_rgb = _params["resolution_rgb"];
    }

    if (_params.contains("camera_device")) {
      _camera_device = _params["camera_device"];
    }

    if (_params.contains("fps")) {
      _fps = _params["fps"];
    }

    set_video_source(_params["dummy"]);
    cout << "\033[1;32mVideo source: " << video_source_to_string(_video_source) << "\033[0m" << endl;
    if (_params.contains("model_file")) {
      _model_file = _params["model_file"];
    } else {
      throw invalid_argument("ERROR: Missing model_file parameter");
    }

    if(setup_video_capture(_params["debug"]["setup_video_capture"]) == return_type::error) {
      cout << "\033[1;31mFailed to setup video capture\033[0m" << endl;
      return;
    }

    if (setup_camera_extrinsics(_params["calibration"],_params["debug"]["coordinate_transform"]) == return_type::error) {
      cout << "\033[1;31mFailed to setup camera extrinsics\033[0m" << endl;
      return;
    }

    setup_OpenPoseModel();
    setup_Pipeline();

    // Show _rgb and _rgbd images in two separate windows contempouraneously
    cv::namedWindow("RGB Frame", cv::WINDOW_NORMAL);
    cv::namedWindow("RGBD Frame", cv::WINDOW_NORMAL);

  } 

  // Implement this method if you want to provide additional information
  map<string, string> info() override { 
    map<string, string> info;
    info["kind"] = kind();
    info["model_file"] = _model_file;
    return info; 
  };

protected:

  string _agent_id; /**< the agent ID */
  string _model_file; /**< the model file path */
  chrono::steady_clock::time_point _frame_time; /**< the timestamp in UNIX [ns] of the last acquired frame */
  int _camera_device = 0;
  data_t _fps = 25;
  
  Mat _rgbd;          /**< the last RGBD frame */
  Mat _rgb;           /**< the last RGB frame */
  Mat _rgbd_filtered; /**< the last RGBD frame filtered with the body index mask*/

  map<string, vector<unsigned char>> _skeleton2D; /**< the skeleton from 2D cameras only*/
  map<string, vector<float>> _skeleton3D;       /**< the skeleton from 3D cameras only*/
  map<string, Eigen::Matrix3f> _cov3D; /**< the 3D covariance matrix */      
  
  vector<Mat> _heatmaps; /**< the joints heatmaps */
  Mat _point_cloud;      /**< the filtered body point cloud */

  std::vector<Eigen::Matrix2f> _cov2D_vec; /**< the 2D covariance matrix vector */
  std::vector<Eigen::Matrix3f> _cov3D_vec; /**< the 3D covariance matrix vector */
  json _params;   /**< the parameters of the plugin */

  cv::Point3f _keypoints_list_azure[18]; /**< the 3D keypoints list for Azure */

  VideoSource _video_source; /**< the video source type */

  #ifdef __linux
  lccv::PiCamera _raspi_rgb_camera; // for Raspi
  #endif

  #ifdef KINECT_AZURE_LIBS
  int _azure_device = 0; /**< the azure device ID */
  k4a_device_configuration_t _device_config; /**< the Kinect Azure device configuration */
  k4a::device _kinect_device; /**< the Kinect Azure device */
  k4a::calibration _kinect_calibration; /**< the Kinect Azure calibration object */
  k4abt::tracker _kinect_tracker; /**< the Kinect Azure tracker */
  k4abt_tracker_configuration_t _kinect_tracker_config; /**< the Kinect Azure tracker configuration */
  k4a::capture _k4a_rgbd_capture; /**< the Kinect Azure RGBD capture */
  k4a_capture_t _kinect_mkv_capture_handle;
  k4a::image _k4a_depth_image; /**< the Kinect Azure depth image */
  k4a::image _k4a_color_image; /**< the Kinect Azure color image */
  k4a::image _body_index_map; /**< the Kinect Azure body index map */
  k4abt::frame _body_frame; /**< the Kinect Azure body frame */
  k4a_transformation_t _point_cloud_transformation; /**< the Kinect Azure point cloud transformation */
  k4a_transformation_t _mkv_color_transformation_handle; /**< the Kinect Azure MKV color transformation handle */
  k4a_transformation_t _kinect_color_transformation_handle; /**< the Kinect Azure color transformation handle */
  k4a_calibration_intrinsic_parameters_t _k4a_rgb_intrinsics; /**< the Kinect Azure RGB camera intrinsics */
  k4a_playback_t _kinect_mkv_playback_handle; /**< the Kinect Azure MKV playback handle */
  #endif

  float _cx; /**< the camera principal point x coordinate */
  float _cy; /**< the camera principal point y coordinate */
  float _fx; /**< the camera focal length x coordinate */
  float _fy; /**< the camera focal length y coordinate */
  Eigen::Matrix4f _camera_transformation_matrix; /**< the camera transformation matrix for coordinate system transformation */

  string _resolution_rgb = ""; /**< the resolution of the RGB frame in the format "widthxheight" */
  int _rgb_height; /**< the height of the RGB frame */
  int _rgb_width; /**< the width of the RGB frame */
  VideoCapture _cap; /**< the OpenCV video capture object */
  int _rgb_width_read = 1280;
  int _rgb_height_read = 720;

  vector<cv::Point2i> _keypoints_list_openpose; /**< the 2D keypoints list for OpenPose */
  vector<cv::Point3f> _keypoints_cov_openpose; /**< the 2D covariance for OpenPose keypoints */
  map<int, int> _keypoints_openpose_to_azure; /**< map of the OpenPose keypoints to the Azure keypoints */

  uint32_t _tsize = 0;              /**< target size*/
  data_t _threshold = 0.1;          /**< probability threshold*/
  string _layout = "";              /**< inputs layouts (NCHW, NHWC)*/
  string _inference_device = "CPU"; /**< computation device*/
  uint32_t _nireq = 0;              /**< number of infer requests*/
  string _nstreams = "";            /**< number of streams*/
  uint32_t _nthreads = 0;           /**< number of CPU threads*/
  uint32_t _frames_processed = 0;
  int64_t _frame_num = 0;

  ov::Core _core; /**< the OpenVINO core object */
  unique_ptr<ResultBase> _result; /**< the result of the model inference */
  OutputTransform _output_transform; /**< the output transform object for resizing the RGB frame */
  unique_ptr<ModelBase> _model; /**< the model object for human pose estimation */
  AsyncPipeline *_pipeline; /**< the OpenVINO pipeline for human pose estimation */
  vector<HumanPose> _poses_openpose; /**<  contains all the keypoints of all identified people */

  int _global_frame_counter = 0; /**< global frame counter for dummy */
  map<int, chrono::steady_clock::time_point> _frame_timestamps; /**< frame timestamps for dummy */

};


/*
  ____  _             _             _      _
 |  _ \| |_   _  __ _(_)_ __     __| |_ __(_)_   _____ _ __
 | |_) | | | | |/ _` | | '_ \   / _` | '__| \ \ / / _ \ '__|
 |  __/| | |_| | (_| | | | | | | (_| | |  | |\ V /  __/ |
 |_|   |_|\__,_|\__, |_|_| |_|  \__,_|_|  |_| \_/ \___|_|
                |___/
Enable the class as plugin
*/
INSTALL_SOURCE_DRIVER(HpePlugin, json)


/*
                  _
  _ __ ___   __ _(_)_ __
 | '_ ` _ \ / _` | | '_ \
 | | | | | | (_| | | | | |
 |_| |_| |_|\__,_|_|_| |_|

For testing purposes, when directly executing the plugin
*/
int main(int argc, char const *argv[]) {
  HpePlugin plugin;
  json output, params;

  // Set example values to params
  params["test"] = "value";

  // Set the parameters
  plugin.set_params(&params);

  // Process data
  plugin.get_output(output);

  // Produce output
  cout << "Output: " << output << endl;

  return 0;
}
